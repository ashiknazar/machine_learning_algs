{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bias\n",
    "    - bias refers to the error introduced by approximating a real world problem with a simple model.\n",
    "    - a model with high bias pays little attention to the training data and oversimplifies the problem , leading to underfitting .model fails to capture pattern\n",
    "- Variance\n",
    "    -  variance refers to the error introduced due to a models sensitivity to small fluctuations in the training dataset .high variance means model is more sensitive ,affected by outliers and overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging vs Boosting: A Comprehensive Comparison\n",
    "\n",
    "## Introduction\n",
    "Bagging and Boosting are both powerful **ensemble learning** techniques used to enhance the performance of machine learning models. While both combine multiple base models to create a stronger learner, they do so in very different ways. Below, we explore the key differences, advantages, and use cases for **Bagging** and **Boosting**.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### **1. Bagging (Bootstrap Aggregating)**\n",
    "- **Goal**: Reduce variance and prevent overfitting.\n",
    "- **Method**: \n",
    "  - **Parallel training** of multiple models on **random subsets** of the data.\n",
    "  - Each model is trained on a **bootstrap sample** (randomly drawn subset with replacement) of the training data.\n",
    "  - After training, predictions from all models are combined by **averaging** (for regression) or **voting** (for classification).\n",
    "- **Examples**: Random Forest, Bagged Decision Trees.\n",
    "- **Impact**: Primarily reduces **variance** but does not directly address bias.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Independence**: Models are trained independently, in parallel.\n",
    "- **Final Prediction**: Averaging or voting.\n",
    "- **Focus**: Reduces overfitting by smoothing predictions across multiple models.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Boosting**\n",
    "- **Goal**: Reduce both bias and variance.\n",
    "- **Method**: \n",
    "  - **Sequential training** of models, where each subsequent model corrects the errors made by the previous one.\n",
    "  - Each new model gives more weight to the **misclassified instances** of previous models.\n",
    "  - The final model is a weighted combination of all the models in the ensemble.\n",
    "- **Examples**: AdaBoost, Gradient Boosting (XGBoost, LightGBM, CatBoost).\n",
    "- **Impact**: Reduces both **bias** and **variance**, creating a strong model by correcting weaknesses iteratively.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Sequential Learning**: Models are trained one after the other, each correcting errors.\n",
    "- **Final Prediction**: Weighted sum (for regression) or weighted vote (for classification).\n",
    "- **Focus**: Corrects errors made by prior models, reduces bias, and can increase accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison Table**\n",
    "\n",
    "| **Aspect**               | **Bagging**                                    | **Boosting**                                      |\n",
    "|--------------------------|------------------------------------------------|--------------------------------------------------|\n",
    "| **Training Process**      | Parallel (independent models)                  | Sequential (models build on each other)          |\n",
    "| **Focus**                 | Reduces variance by averaging/voting           | Reduces both bias and variance by correcting errors|\n",
    "| **Model Weighting**       | Equal weights for all models                   | Models are weighted based on performance         |\n",
    "| **Combination of Models** | Averaging (regression) or Voting (classification) | Weighted sum (regression) or weighted vote (classification) |\n",
    "| **Examples**              | Random Forest                                  | AdaBoost, Gradient Boosting (XGBoost, LightGBM)  |\n",
    "| **Risk of Overfitting**   | Less prone (reduces variance)                  | More prone (especially with noisy data)          |\n",
    "| **Computational Cost**    | Faster to train (independent models)           | Slower to train (sequential nature)              |\n",
    "| **Parallelization**       | Easily parallelizable                          | Difficult to parallelize due to sequential nature|\n",
    "| **Handling Noise**        | More robust to noise and outliers              | Can be sensitive to noisy data                   |\n",
    "\n",
    "---\n",
    "\n",
    "## **When to Use Each?**\n",
    "\n",
    "### **Use Bagging if:**\n",
    "- Your model is prone to overfitting due to high variance.\n",
    "- You have noisy data or outliers that might negatively affect your model.\n",
    "- You want a stable, less complex model (e.g., Random Forest).\n",
    "- You want to combine models independently to reduce overfitting.\n",
    "\n",
    "### **Use Boosting if:**\n",
    "- You want to improve predictive accuracy and reduce both bias and variance.\n",
    "- Your base learner is weak (e.g., shallow decision trees) and you want to correct errors.\n",
    "- You are willing to take on additional computational complexity for potentially higher accuracy.\n",
    "- You need to focus on difficult, misclassified instances in your dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **Pros and Cons**\n",
    "\n",
    "### **Bagging**\n",
    "- **Pros**:\n",
    "  - Reduces variance, preventing overfitting.\n",
    "  - Handles noisy data and outliers well.\n",
    "  - Simple and intuitive approach.\n",
    "- **Cons**:\n",
    "  - Does not reduce bias (can still have high bias if base learner is weak).\n",
    "  - May not outperform boosting on certain problems.\n",
    "\n",
    "### **Boosting**\n",
    "- **Pros**:\n",
    "  - Often results in higher accuracy by reducing both bias and variance.\n",
    "  - Can be used with weak learners, improving their performance significantly.\n",
    "  - Works well for complex datasets.\n",
    "- **Cons**:\n",
    "  - Prone to overfitting, especially with noisy data.\n",
    "  - Computationally expensive and harder to parallelize.\n",
    "  - Sensitive to outliers, as misclassified instances are given more weight.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- **Bagging** is a technique for reducing **variance** by training models independently on different subsets of the data and combining their predictions. It is most useful for models that are highly complex and prone to overfitting (e.g., decision trees).\n",
    "- **Boosting**, on the other hand, reduces **bias** and **variance** by focusing sequentially on the mistakes of previous models. It is highly effective in improving the accuracy of weak learners but can be prone to overfitting and is computationally more expensive.\n",
    "\n",
    "Choose **Bagging** when stability and reducing variance are your priorities, and opt for **Boosting** when improving accuracy by focusing on difficult instances and reducing both bias and variance is your goal.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient boosting\n",
    "![grad_data](images/grad_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let base model is to find average of y \n",
    "- let it is 75k \n",
    "- compute residual\n",
    "![](images/grad2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let the difference is the residual here -25,-5,...\n",
    "-  construct decision tree with old inputs and output R1\n",
    "-  get result and residuals\n",
    "- for input 1 -> 75 + lr (R2) is output\n",
    "- add one more decision tree , here output will be R2\n",
    "- output ->F(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ada](images/ada1.png)\n",
    "![ada](images/ada2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- boosting\n",
    "    - create base learners sequentially\n",
    "    - if n records are incorrectly classified ,only these records are passed to next base learner. and so on ...\n",
    "    - it will go on unless and until we want only some amount of base learners\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABOOST\n",
    "- we have  weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step1\n",
    "- let f1,f2,f3 be features and we have 7 rows\n",
    "- assign sample weight to each row 1/7 (1/n)\n",
    "#### step2\n",
    "- first base learner will be decision tree\n",
    "    - here decistion trees are of a single depth ->stumps\n",
    "    - we create a stump foreach feature and choose one with low entropy as first base learner\n",
    "#### step3\n",
    "- if it predicted 4 correct and 1 wrong \n",
    "- we will calculate total error(TE) =  1/7 (add all sample weights)\n",
    "- find performance of stump = 1/2 log_e(1-TE)/TE = 0.896\n",
    "- only wrong classified record will be passed to next baselearner\n",
    "- for that weight of wrongly classified record will be increased and others will be dicreased\n",
    "  - new weight (erronious data)= weight * e^performance say  (.896)\n",
    "  - for correclty classified points  change formula by adding '- ' to performance say\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4\n",
    "- total of updated weights is not 1 so standardize it\n",
    "- create new dataset \n",
    "    - based on normalized weight we will create bucket\n",
    "      - let (0.07,0.51,0.07,0.07,0.07,0.07,0.07) be updated weights\n",
    "      - 0 to 0.07 is first bucket\n",
    "      - 0.07 to 0.51 be next bucket\n",
    "      - 0.51 to 0.51+0.07 be next bucket  and so on\n",
    "    - iterativly choose datasets .suppose first iteration choosen 0.43 select the bucket and curresponding data and populate to new  \n",
    "    - probability of choosing erronious data will be hight\n",
    "    - create new stump and continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
