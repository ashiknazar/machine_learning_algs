{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data processing is a crucial step in any machine learning project. It involves transforming raw data into a format that can be used for analysis, modeling, and making predictions. This step can significantly impact the quality of the machine learning model. The following outlines the common steps involved in data processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Description: Gathering raw data from different sources such as databases, APIs, flat files (CSV, Excel), web scraping, or sensors.\n",
    "- Tools: APIs (e.g., requests, beautifulsoup), Databases (e.g., SQL, MongoDB), Files (e.g., pandas.read_csv())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data cleaning is the process of handling missing, inconsistent, or outlier data that can affect model accuracy. This step ensures that the data is accurate and ready for analysis.\n",
    "\n",
    "#### 1.Handling Missing Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Imputation</b>:Fill missing values with the mean, median, or mode (for numerical data) or the most frequent value (for categorical data).\n",
    "-  <b> Deletion</b>: Remove rows or columns with missing values (use with caution, as it might lose important information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Removing Duplicates:\n",
    "- pandas.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Handling Outliers:\n",
    "- Outliers can skew results and make the model less accurate.\n",
    "- Use statistical methods (e.g., Z-scores, IQR) to identify outliers.\n",
    "- Tools: pandas, scipy.stats (Z-scores), numpy.percentile (IQR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##  Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EDA is used to understand the structure of the dataset and the relationships between different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization: \n",
    "\n",
    "- Plotting graphs to understand the distribution and relationships.\n",
    "   - matplotlib, seaborn, plotly.\n",
    "   - Histograms to understand the distribution of a feature.\n",
    "   - Scatter plots to understand the relationship between two features.\n",
    "   - Boxplots to visualize the spread and detect outliers.\n",
    "#### Statistical Summaries: \n",
    "- Getting the central tendency (mean, median, mode), spread (variance, standard deviation), and other summary statistics.\n",
    "   - pandas.describe(), scipy.stats\n",
    "#### Correlation Analysis:\n",
    "   - Check for correlations between features. Strong correlations may suggest multicollinearity or redundancy.\n",
    "     - pandas.corr(), seaborn.heatmap()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##  Feature Engineering\n",
    " Feature engineering involves transforming raw data into a set of features that better represent the underlying problem to the model.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating New Features:\n",
    "- Based on domain knowledge or mathematical transformations, new features can be derived\n",
    "#### Categorical Encoding:\n",
    "- Machine learning models typically require numerical input. Categorical variables must be converted into numerical representations. \n",
    "    - One-Hot Encoding:  pandas.get_dummies(), sklearn.preprocessing.OneHotEncoder().\n",
    "    - [Label Encoding](#section1) :Assign an integer value to each category (useful for ordinal features).\n",
    "    - [Target Encoding](#section2) : Replace categories with the mean of the target variable for each category.\n",
    "#### Feature Scaling:\n",
    "-  Standardize or normalize the features to bring them onto the same scale, especially important for distance-based models like KNN or gradient descent-based models like logistic regression.\n",
    "- [Normalization](#section3):  Rescaling features to a range (typically [0, 1]).sklearn.preprocessing.MinMaxScaler().\n",
    "<br><br>\n",
    "- [Standardization](#section4): Scaling features to have zero mean and unit variance.sklearn.preprocessing.StandardScaler().\n",
    "#### Feature Selection: \n",
    "- Remove irrelevant or redundant features.\n",
    "- Correlation Thresholding: Remove features that are highly correlated with each other.\n",
    "- Univariate Feature Selection: Select features based on their statistical significance with the target variable.\n",
    "- Recursive Feature Elimination (RFE): A method to iteratively remove features and build the model to select the most important ones.\n",
    "-sklearn.feature_selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data transformation refers to modifying or converting data in a way that makes it more suitable for the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Log Transformation:</b> Apply logarithmic transformation to features that have a skewed distribution.\n",
    "- <b>Polynomial Features:</b> Create interaction terms or higher-degree polynomial features.\n",
    "- <b>Dimensionality Reduction:</b> Reduce the number of features to a lower-dimensional space (especially useful when there are many features).\n",
    "   -  PCA (Principal Component Analysis): Projects data onto fewer dimensions while preserving as much variance as possible.\n",
    "   -  t-SNE (t-Distributed Stochastic Neighbor Embedding): For visualizing high-dimensional data in 2D or 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Data Spitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set: \n",
    "- Used to train the machine learning model.\n",
    "#### Test Set:\n",
    "- Used to evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Model Training\n",
    "- Once the data is clean, transformed, and split, you can train a machine learning model using the training dataset.\n",
    "- Examples of models include regression models, classification models, or clustering algorithms, depending on the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##  Model Evaluation\n",
    "- fter training the model, evaluate its performance using various metrics based on the type of problem (regression, classification, etc.).\n",
    "- For Regression: Common metrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared \n",
    "- For Classification: Common metrics include Accuracy, Precision, Recall, F1-Score, AUC-ROC.\n",
    "\n",
    "__\n",
    "##  Model Tuning\n",
    "\n",
    "- Hyperparameter tuning can be done to improve the model performance by selecting the best hyperparameters (e.g., learning rate, number of trees in a random forest, etc.).\n",
    "  - Grid Search: Exhaustively tries all possible hyperparameter combinations.\n",
    "  - Random Search: Randomly tries different hyperparameter combinations.\n",
    "  - Bayesian Optimization: A probabilistic model to select hyperparameters that are most likely to improve the model.\n",
    "___\n",
    "##  Deployment\n",
    " - After training and evaluation, the model is deployed into a production environment, where it can be used to make real-time predictions on new data.\n",
    " - Tools: Flask, Django (for APIs), Docker (for containerization), cloud platforms (AWS, GCP, Azure).\n",
    " ___\n",
    " ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"section1\"><u>Label encoding</u></h2>\n",
    "- Red is encoded as 0 <br>\n",
    "- Green is encoded as 1<br>\n",
    "- Blue is encoded as 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color  Color_encoded\n",
      "0    Red              2\n",
      "1  Green              1\n",
      "2   Blue              0\n",
      "3  Green              1\n",
      "4   Blue              0\n",
      "5    Red              2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data\n",
    "data = {'Color': ['Red', 'Green', 'Blue', 'Green', 'Blue', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to the 'Color' column\n",
    "df['Color_encoded'] = label_encoder.fit_transform(df['Color'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<h2 id=\"section2\"><u>Target encoding</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Target Encoding (also known as Mean Encoding) is a technique where categorical values are replaced by the mean of the target variable (dependent variable) for each category. It can be useful when dealing with categorical features with a high cardinality (many categories), and is often used in situations where One-Hot Encoding would create a sparse matrix with too many columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 .Calculate the mean of the target variable for each category in the feature.\n",
    "- 2 .\n",
    "Replace each category in the feature with the corresponding mean of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose you have a dataset where you're predicting whether a person will buy a product (target variable Purchase, 1 for Yes and 0 for No) based on the City they live in (categorical feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       City  Purchase  City_encoded\n",
      "0  New York         1           0.5\n",
      "1    Boston         0           0.5\n",
      "2   Chicago         1           0.5\n",
      "3    Boston         1           0.5\n",
      "4  New York         0           0.5\n",
      "5   Chicago         0           0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'City': ['New York', 'Boston', 'Chicago', 'Boston', 'New York', 'Chicago'],\n",
    "    'Purchase': [1, 0, 1, 1, 0, 0]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the target mean for each category in 'City'\n",
    "target_encoding = df.groupby('City')['Purchase'].mean()\n",
    "\n",
    "# Map the target mean to the 'City' column\n",
    "df['City_encoded'] = df['City'].map(target_encoding)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"section3\"><u>Normalization</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalization, often called Min-Max Scaling, rescales the features to a specific range, usually [0, 1] or [-1, 1]. The key idea is to subtract the minimum value of the feature and then divide by the range (the difference between the maximum and minimum values). This method is useful when you want to ensure that all features have the same scale and are bounded within a specific range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you want the values of your data to fall within a specific range (like [0, 1]).\n",
    "\n",
    "- Especially useful for machine learning algorithms that rely on the distance between data points, such as:\n",
    "\n",
    "  - K-Nearest Neighbors (KNN)\n",
    "  - Neural Networks\n",
    "  - Support Vector Machines (SVM) with RBF kernel\n",
    "- Normalization can also help when you're dealing with non-Gaussian distributions or when features have different units or very different ranges (e.g., height in cm and weight in kg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.33333333 0.33333333]\n",
      " [0.66666667 0.66666667]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply normalization\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "print(normalized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (Min-Max Scaling)\n",
    "\n",
    "Normalization rescales the data into a fixed range, typically [0, 1]. The formula for normalization is:\n",
    "\n",
    "$$\n",
    "X_{\\text{norm}} = \\frac{X - \\min(X)}{\\max(X) - \\min(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the original feature value.\n",
    "- $\\min(X)$ is the minimum value of the feature.\n",
    "- $\\max(X)$ is the maximum value of the feature.\n",
    "- $X_{\\text{norm}}$ is the normalized value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"section4\"><u>Standardization</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standardization, also known as Z-score normalization, rescales the features so that they have a mean of 0 and a standard deviation of 1. It is the most common method for scaling features and is often preferred when working with many machine learning algorithms, especially those that rely on Gaussian assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization (Z-score Normalization)\n",
    "\n",
    "Standardization rescales the data so that it has a mean of 0 and a standard deviation of 1. The formula for standardization is:\n",
    "\n",
    "$$\n",
    "X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the original feature value.\n",
    "- $\\mu$ (mu) is the mean of the feature.\n",
    "- $\\sigma$ (sigma) is the standard deviation of the feature.\n",
    "- $X_{\\text{std}}$ is the standardized value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z score\n",
    "- In machine learning, the Z-score is a statistical measure that describes how many standard deviations a data point is from the mean of a dataset. It's a way to standardize or normalize data, making it easier to compare values that come from different distributions or have different scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Z-scores are used in Machine Learning\n",
    "- Feature Scaling: In many machine learning algorithms (like k-Nearest Neighbors, Support Vector Machines, or neural networks), the model performance can be sensitive to the scale of the data. Features with different units or vastly different scales can cause some features to dominate others, leading to suboptimal performance. Standardizing features using Z-scores (a process called z-score normalization) ensures that each feature contributes equally to the model.\n",
    "\n",
    "- Outlier Detection: Z-scores help identify outliers. A Z-score significantly higher or lower than 0 indicates that the data point is far away from the mean. Common thresholds are Z-scores of ±2 or ±3, which correspond to data points that are more than 2 or 3 standard deviations away from the mean, respectively.\n",
    "\n",
    "- Assumption for Algorithms: Many machine learning models assume or perform better when the data is approximately normally distributed (Gaussian distribution). Z-score normalization can help make data more Gaussian-like by adjusting for extreme values and centering the data around 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
