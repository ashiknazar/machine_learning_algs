{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mathematical Calculations Related to Finding Accuracy of Machine Learning Algorithms**\n",
    "\n",
    "In machine learning, evaluating the performance of a model is crucial to understanding how well it generalizes to unseen data. Several mathematical metrics are used to assess the accuracy and effectiveness of models, especially for classification tasks. Below are the key metrics and calculations related to the accuracy of machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Accuracy**\n",
    "\n",
    "**Definition**:\n",
    "Accuracy is one of the simplest and most commonly used metrics for evaluating classification models. It measures the proportion of correct predictions made by the model.\n",
    "\n",
    "### Formula:\n",
    "The accuracy is calculated as the ratio of correct predictions to the total number of predictions:\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} $$\n",
    "\n",
    "Alternatively, in terms of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN):\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "Where:\n",
    "- **TP (True Positive)**: Correctly predicted positive cases\n",
    "- **TN (True Negative)**: Correctly predicted negative cases\n",
    "- **FP (False Positive)**: Incorrectly predicted positive cases (Type I error)\n",
    "- **FN (False Negative)**: Incorrectly predicted negative cases (Type II error)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Precision**\n",
    "\n",
    "**Definition**:\n",
    "Precision (also called Positive Predictive Value) measures the proportion of positive predictions that are actually correct. It is particularly important in situations where false positives are costly (e.g., fraud detection).\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Recall (Sensitivity or True Positive Rate)**\n",
    "\n",
    "**Definition**:\n",
    "Recall (also called Sensitivity or True Positive Rate) measures the proportion of actual positive cases that were correctly identified by the model. It is useful when false negatives are critical (e.g., in medical diagnosis).\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "---\n",
    "\n",
    "## **4. F1-Score**\n",
    "\n",
    "**Definition**:\n",
    "The F1-score is the harmonic mean of precision and recall. It is a single metric that combines both precision and recall into one score. It is particularly useful when you need a balance between precision and recall.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "$$ \\text{F1-Score} = 2 \\times \\frac{TP}{2TP + FP + FN} $$\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Specificity (True Negative Rate)**\n",
    "\n",
    "**Definition**:\n",
    "Specificity measures the proportion of actual negative cases that are correctly identified. It is useful in scenarios where false positives need to be minimized.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{Specificity} = \\frac{TN}{TN + FP} $$\n",
    "\n",
    "---\n",
    "\n",
    "## **6. ROC Curve and AUC (Area Under the Curve)**\n",
    "\n",
    "**Definition**:\n",
    "The **Receiver Operating Characteristic (ROC)** curve is a graphical representation of the true positive rate (recall) against the false positive rate. The **Area Under the Curve (AUC)** provides an aggregate measure of the model’s ability to discriminate between classes.\n",
    "\n",
    "### Formula:\n",
    "The **false positive rate (FPR)** and **true positive rate (TPR)** are calculated as follows:\n",
    "\n",
    "$$ \\text{FPR} = \\frac{FP}{FP + TN} $$\n",
    "\n",
    "$$ \\text{TPR} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "The **AUC** is the area under the ROC curve and represents the model's ability to rank positive instances higher than negative ones. An AUC of 0.5 means the model is no better than random guessing, while an AUC of 1.0 indicates perfect performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Matthews Correlation Coefficient (MCC)**\n",
    "\n",
    "**Definition**:\n",
    "The Matthews Correlation Coefficient (MCC) is a more informative metric that balances all four confusion matrix categories. It is especially useful when dealing with imbalanced datasets.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{MCC} = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} $$\n",
    "\n",
    "The MCC returns a value between -1 and 1:\n",
    "- **1**: Perfect prediction\n",
    "- **0**: No better than random prediction\n",
    "- **-1**: Completely wrong prediction\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Logarithmic Loss (Log Loss)**\n",
    "\n",
    "**Definition**:\n",
    "Log Loss measures the performance of a classification model where the output is a probability value between 0 and 1. It calculates the penalty for incorrect classifications with a higher penalty for more confident but incorrect predictions.\n",
    "\n",
    "### Formula:\n",
    "For a binary classification, Log Loss is given as:\n",
    "\n",
    "$$ \\text{Log Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right] $$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the true label (0 or 1)\n",
    "- \\( p_i \\) is the predicted probability of the positive class for sample \\( i \\)\n",
    "- \\( N \\) is the total number of samples\n",
    "\n",
    "A lower log loss indicates better model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Confusion Matrix**\n",
    "\n",
    "**Definition**:\n",
    "The confusion matrix is a table used to evaluate the performance of a classification model. It summarizes the true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "### Formula:\n",
    "The matrix structure is:\n",
    "\n",
    "|              | Predicted Positive | Predicted Negative |\n",
    "|--------------|--------------------|--------------------|\n",
    "| **Actual Positive** | TP                 | FN                 |\n",
    "| **Actual Negative** | FP                 | TN                 |\n",
    "\n",
    "From this matrix, various metrics like accuracy, precision, recall, F1-score, and others can be derived.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Hamming Loss**\n",
    "\n",
    "**Definition**:\n",
    "Hamming Loss is used for multi-label classification problems, and it measures the fraction of incorrect labels across all samples.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{Hamming Loss} = \\frac{1}{N \\cdot L} \\sum_{i=1}^{N} \\sum_{j=1}^{L} \\mathbb{I}(y_{ij} \\neq \\hat{y}_{ij}) $$\n",
    "\n",
    "Where:\n",
    "- \\( N \\) is the number of samples\n",
    "- \\( L \\) is the number of labels\n",
    "- \\( \\mathbb{I}(y_{ij} \\neq \\hat{y}_{ij}) \\) is an indicator function that is 1 if the true label is not equal to the predicted label for the \\( j \\)-th label of the \\( i \\)-th sample, and 0 otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "## **11. Mean Absolute Error (MAE)**\n",
    "\n",
    "**Definition**:\n",
    "For regression tasks, Mean Absolute Error (MAE) measures the average of the absolute errors between predicted and actual values.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i| $$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the actual value\n",
    "- \\( \\hat{y}_i \\) is the predicted value\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "These mathematical metrics are essential for evaluating machine learning models. While **accuracy** is often the most straightforward measure, it can be misleading in imbalanced datasets. Other metrics like **precision**, **recall**, **F1-score**, **AUC**, and **MCC** provide a more nuanced understanding of model performance, especially when dealing with imbalanced data or specific error costs. It's important to choose the right evaluation metric based on the problem at hand to get a true sense of how well the model performs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Additional Evaluation Metrics for Machine Learning Algorithms**\n",
    "\n",
    "In addition to the commonly used classification metrics like accuracy, precision, recall, and F1-score, there are several other evaluation metrics used for regression tasks and general performance assessment. Below are some of these important metrics, including Mean Squared Error (MSE), R-squared (R²), and Adjusted R-squared, among others.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Mean Squared Error (MSE)**\n",
    "\n",
    "**Definition**:\n",
    "Mean Squared Error (MSE) is a commonly used metric for evaluating regression models. It calculates the average squared difference between the predicted and actual values. MSE gives a high penalty to large errors due to squaring the differences.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the actual value\n",
    "- \\( \\hat{y}_i \\) is the predicted value\n",
    "- \\( N \\) is the number of samples\n",
    "\n",
    "A lower MSE indicates better model performance, with perfect predictions yielding an MSE of 0.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Root Mean Squared Error (RMSE)**\n",
    "\n",
    "**Definition**:\n",
    "Root Mean Squared Error (RMSE) is the square root of the Mean Squared Error. It provides the error in the same units as the original data, making it easier to interpret.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the actual value\n",
    "- \\( \\hat{y}_i \\) is the predicted value\n",
    "- \\( N \\) is the number of samples\n",
    "\n",
    "---\n",
    "\n",
    "## **3. R-squared (R²)**\n",
    "\n",
    "**Definition**:\n",
    "R-squared, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It is a measure of how well the regression model fits the data.\n",
    "\n",
    "### Formula:\n",
    "$$ R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2} $$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the actual value\n",
    "- \\( \\hat{y}_i \\) is the predicted value\n",
    "- \\( \\bar{y} \\) is the mean of the actual values\n",
    "- \\( N \\) is the number of samples\n",
    "\n",
    "- **Interpretation**:\n",
    "  - \\( R^2 = 1 \\): Perfect fit\n",
    "  - \\( R^2 = 0 \\): No fit (the model does not explain any of the variance in the target variable)\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Adjusted R-squared (Adjusted R²)**\n",
    "\n",
    "**Definition**:\n",
    "Adjusted R-squared adjusts the R-squared value for the number of predictors in the model. It provides a more accurate measure when comparing models with different numbers of predictors, as it penalizes adding irrelevant features.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\times \\frac{N - 1}{N - p - 1} $$\n",
    "\n",
    "Where:\n",
    "- \\( N \\) is the number of samples\n",
    "- \\( p \\) is the number of predictors (independent variables)\n",
    "- \\( R^2 \\) is the R-squared value\n",
    "\n",
    "- **Interpretation**:\n",
    "  - A higher Adjusted \\( R^2 \\) indicates a better fit, but unlike R-squared, it takes into account the number of features used in the model.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Mean Absolute Percentage Error (MAPE)**\n",
    "\n",
    "**Definition**:\n",
    "Mean Absolute Percentage Error (MAPE) is a commonly used metric for regression tasks that measures the average absolute percentage difference between the predicted and actual values.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{MAPE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right| \\times 100 $$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the actual value\n",
    "- \\( \\hat{y}_i \\) is the predicted value\n",
    "- \\( N \\) is the number of samples\n",
    "\n",
    "- **Interpretation**:\n",
    "  - A lower MAPE indicates better model performance. It is particularly useful when the errors need to be interpreted in percentage terms.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Explained Variance Score**\n",
    "\n",
    "**Definition**:\n",
    "Explained Variance Score measures the proportion of the variance in the target variable that is explained by the model. It is similar to R-squared but focuses on how much of the total variance is explained by the predictions.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{Explained Variance} = 1 - \\frac{\\text{Variance of residuals}}{\\text{Variance of the actual values}} $$\n",
    "\n",
    "Where the residuals are the differences between the actual and predicted values.\n",
    "\n",
    "- **Interpretation**:\n",
    "  - A score close to 1 indicates a model that explains most of the variance in the data, while a score close to 0 indicates the model explains very little.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Huber Loss**\n",
    "\n",
    "**Definition**:\n",
    "Huber Loss is a loss function used in regression that combines the benefits of both Mean Squared Error (MSE) and Mean Absolute Error (MAE). It is less sensitive to outliers than MSE.\n",
    "\n",
    "### Formula:\n",
    "For each error \\( \\delta = |y_i - \\hat{y}_i| \\), the Huber loss is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Huber Loss} =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2} \\delta^2 & \\text{for} \\, \\delta \\leq \\delta_{\\text{threshold}} \\\\\n",
    "\\delta_{\\text{threshold}}(\\delta - \\frac{1}{2} \\delta_{\\text{threshold}}) & \\text{for} \\, \\delta > \\delta_{\\text{threshold}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where \\( \\delta_{\\text{threshold}} \\) is a user-defined threshold value.\n",
    "\n",
    "- **Interpretation**:\n",
    "  - Huber Loss behaves like MSE for small errors and like MAE for large errors, making it robust to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. AIC (Akaike Information Criterion)**\n",
    "\n",
    "**Definition**:\n",
    "AIC is a model evaluation criterion that helps in model selection. It penalizes models for having too many parameters, which can lead to overfitting.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{AIC} = 2k - 2 \\ln(L) $$\n",
    "\n",
    "Where:\n",
    "- \\( k \\) is the number of parameters in the model\n",
    "- \\( L \\) is the likelihood of the model\n",
    "\n",
    "- **Interpretation**:\n",
    "  - Lower AIC values indicate a better model. AIC helps to balance the goodness of fit with model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. BIC (Bayesian Information Criterion)**\n",
    "\n",
    "**Definition**:\n",
    "BIC is similar to AIC, but it applies a larger penalty for models with more parameters. It is particularly useful when comparing models with different sample sizes.\n",
    "\n",
    "### Formula:\n",
    "$$ \\text{BIC} = \\ln(N)k - 2 \\ln(L) $$\n",
    "\n",
    "Where:\n",
    "- \\( N \\) is the number of samples\n",
    "- \\( k \\) is the number of parameters in the model\n",
    "- \\( L \\) is the likelihood of the model\n",
    "\n",
    "- **Interpretation**:\n",
    "  - Like AIC, lower BIC values indicate better models. BIC is especially useful when the number of samples is large.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Theil’s U-statistic**\n",
    "\n",
    "**Definition**:\n",
    "Theil’s U-statistic measures how well the model's predictions match the true values, similar to MSE but with a focus on proportional differences.\n",
    "\n",
    "### Formula:\n",
    "$$ U = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{y_i - \\hat{y}_i}{y_i} \\right)^2} $$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the actual value\n",
    "- \\( \\hat{y}_i \\) is the predicted value\n",
    "- \\( N \\) is the number of samples\n",
    "\n",
    "- **Interpretation**:\n",
    "  - The closer U is to 0, the better the model's predictions match the actual values. \n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "In regression and other tasks, these additional metrics, such as **MSE**, **R-squared**, **Adjusted R-squared**, **MAPE**, and **AIC**, provide deeper insights into the model's performance. By choosing the appropriate evaluation metric, you can assess the predictive quality of your model more accurately and ensure it is well-suited to your specific problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
