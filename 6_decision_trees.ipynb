{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Trees**\n",
    "\n",
    "A **Decision Tree** is a popular supervised learning algorithm used for both classification and regression tasks. It splits the data into subsets based on feature values, creating a tree-like model of decisions. The goal of a decision tree is to classify data by learning simple decision rules inferred from the features.\n",
    "\n",
    "---\n",
    "\n",
    "## **Basic Concepts**\n",
    "\n",
    "- **Node**: A decision node represents a feature or attribute, where the data is split based on certain conditions.\n",
    "- **Leaf Node**: A leaf node represents the final decision or outcome (e.g., class label for classification or a continuous value for regression).\n",
    "- **Root Node**: The root node is the topmost node of the tree, representing the feature that provides the best split at the initial level.\n",
    "- **Branches**: Branches connect nodes, representing the outcome of a decision made at a parent node.\n",
    "\n",
    "---\n",
    "\n",
    "## **Working of Decision Trees**\n",
    "\n",
    "### **1. Splitting the Data**\n",
    "\n",
    "The decision tree algorithm recursively splits the dataset into smaller subsets. The split is based on features that result in the best separation of data. The goal is to reduce the **impurity** of the data at each split. There are different metrics used to calculate the \"best\" split.\n",
    "\n",
    "### **2. Stopping Criterion**\n",
    "\n",
    "The algorithm continues splitting until:\n",
    "- The tree reaches a specified depth.\n",
    "- The data points in a node are pure (i.e., all belong to the same class).\n",
    "- There is no further information gain from splitting.\n",
    "\n",
    "---\n",
    "\n",
    "## **Splitting Criteria**\n",
    "\n",
    "To determine the best feature to split on, the decision tree algorithm evaluates the following measures:\n",
    "\n",
    "### **1. Gini Impurity (for Classification)**\n",
    "\n",
    "The **Gini Impurity** measures how often a randomly selected element would be incorrectly classified. The formula is:\n",
    "\n",
    "$$\n",
    "\\text{Gini}(D) = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( p_i \\) is the proportion of class \\( i \\) in the dataset.\n",
    "- \\( C \\) is the number of classes.\n",
    "\n",
    "The goal is to choose the feature that results in the lowest Gini impurity.\n",
    "\n",
    "### **2. Entropy and Information Gain (for Classification)**\n",
    "\n",
    "**Entropy** measures the uncertainty or disorder in a dataset. It is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Entropy}(D) = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( p_i \\) is the proportion of class \\( i \\) in the dataset.\n",
    "\n",
    "**Information Gain** is the reduction in entropy when a feature is used to split the dataset. It is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Information Gain} = \\text{Entropy}(D) - \\sum_{v \\in V} \\frac{|D_v|}{|D|} \\text{Entropy}(D_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( D_v \\) represents the subset of data where the feature \\( v \\) has a particular value.\n",
    "\n",
    "The feature that maximizes information gain is selected for the split.\n",
    "\n",
    "### **3. Mean Squared Error (MSE) (for Regression)**\n",
    "\n",
    "For regression tasks, the algorithm uses the **Mean Squared Error (MSE)** as the impurity measure:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(D) = \\frac{1}{|D|} \\sum_{i=1}^{|D|} (y_i - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the actual value of the target variable.\n",
    "- \\( \\hat{y} \\) is the predicted value (mean of the target values in the node).\n",
    "\n",
    "The algorithm tries to minimize the MSE when choosing the best split.\n",
    "\n",
    "---\n",
    "\n",
    "## **Building a Decision Tree**\n",
    "\n",
    "1. **Choose the Best Feature**: At each node, evaluate each feature and calculate the Gini Impurity, Entropy, or MSE for all possible splits.\n",
    "2. **Split the Data**: Based on the chosen feature, divide the dataset into subsets.\n",
    "3. **Repeat**: Recursively apply the process to each subset, building the tree from the top (root) down to the leaf nodes.\n",
    "4. **Stop Criteria**: The recursion ends when the data in a node is pure or when the tree reaches a predefined maximum depth.\n",
    "\n",
    "---\n",
    "\n",
    "## **Pruning**\n",
    "\n",
    "**Pruning** is the process of removing unnecessary branches from the decision tree to avoid overfitting. There are two types of pruning:\n",
    "\n",
    "### **1. Pre-Pruning (Early Stopping)**\n",
    "\n",
    "Pre-pruning involves stopping the tree-building process early before it becomes too complex. This can be done by limiting:\n",
    "- The maximum depth of the tree.\n",
    "- The minimum number of samples required to split a node.\n",
    "- The minimum improvement in the splitting criterion.\n",
    "\n",
    "### **2. Post-Pruning (Cost Complexity Pruning)**\n",
    "\n",
    "Post-pruning involves first building the full tree and then trimming branches that have little importance. This can be done by evaluating the performance of the tree on a validation set and removing branches that don't contribute much to improving accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of Decision Trees**\n",
    "\n",
    "- **Easy to understand and interpret**: Decision trees are easy to visualize and understand, even for non-experts.\n",
    "- **No need for feature scaling**: Decision trees do not require normalization or standardization of data.\n",
    "- **Handles both numerical and categorical data**: Decision trees can handle different types of features without the need for preprocessing.\n",
    "- **Non-linear relationships**: Decision trees can capture complex, non-linear relationships between features.\n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages of Decision Trees**\n",
    "\n",
    "- **Overfitting**: Decision trees can easily overfit, especially with deep trees that fit noise in the data.\n",
    "- **Instability**: Small changes in the data can lead to completely different tree structures.\n",
    "- **Greedy Algorithm**: The decision tree algorithm is greedy, making locally optimal splits without considering the global best solution.\n",
    "- **Bias towards dominant classes**: In imbalanced datasets, decision trees may favor the majority class.\n",
    "\n",
    "---\n",
    "\n",
    "## **Ensemble Methods**\n",
    "\n",
    "To overcome some of the limitations of decision trees, ensemble methods like **Random Forests** and **Gradient Boosting** are often used. These methods combine multiple decision trees to improve accuracy and reduce overfitting.\n",
    "\n",
    "- **Random Forest**: Builds multiple decision trees using different subsets of data and features, and combines their results to improve robustness and accuracy.\n",
    "- **Gradient Boosting**: Builds decision trees sequentially, where each tree corrects the errors of the previous tree.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications**\n",
    "\n",
    "Decision trees are widely used in a variety of fields, including:\n",
    "- **Classification**: Spam detection, fraud detection, medical diagnoses.\n",
    "- **Regression**: Predicting house prices, sales forecasting.\n",
    "- **Feature Selection**: Used in feature engineering for reducing the number of variables in predictive models.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example**\n",
    "\n",
    "Given a dataset with features like **age** and **income**, the decision tree algorithm would:\n",
    "\n",
    "- Start at the root node and ask a question like: \"Is age greater than 40?\"\n",
    "- Based on the answer, it would split the data into two branches (e.g., age <= 40 and age > 40).\n",
    "- Each branch would continue to split based on another feature (e.g., \"Is income > 50k?\").\n",
    "- Eventually, the tree would end at leaf nodes where a class label or regression value is assigned.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "Decision trees are intuitive models used for classification and regression tasks. They split data into subsets based on feature values, aiming to create a simple decision rule for each subset. While they are easy to interpret, they are prone to overfitting and require careful tuning and pruning. Ensemble methods like Random Forests and Gradient Boosting can be used to improve performance and robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
