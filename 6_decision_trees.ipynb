{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Trees**\n",
    "\n",
    "A **Decision Tree** is a popular supervised learning algorithm used for both classification and regression tasks. It splits the data into subsets based on feature values, creating a tree-like model of decisions. The goal of a decision tree is to classify data by learning simple decision rules inferred from the features.\n",
    "\n",
    "---\n",
    "\n",
    "## **Basic Concepts**\n",
    "\n",
    "- **Node**: A decision node represents a feature or attribute, where the data is split based on certain conditions.\n",
    "- **Leaf Node**: A leaf node represents the final decision or outcome (e.g., class label for classification or a continuous value for regression).\n",
    "- **Root Node**: The root node is the topmost node of the tree, representing the feature that provides the best split at the initial level.\n",
    "- **Branches**: Branches connect nodes, representing the outcome of a decision made at a parent node.\n",
    "\n",
    "---\n",
    "\n",
    "## **Working of Decision Trees**\n",
    "\n",
    "### **1. Splitting the Data**\n",
    "\n",
    "The decision tree algorithm recursively splits the dataset into smaller subsets. The split is based on features that result in the best separation of data. The goal is to reduce the **impurity** of the data at each split. There are different metrics used to calculate the \"best\" split.\n",
    "\n",
    "### **2. Stopping Criterion**\n",
    "\n",
    "The algorithm continues splitting until:\n",
    "- The tree reaches a specified depth.\n",
    "- The data points in a node are pure (i.e., all belong to the same class).\n",
    "- There is no further information gain from splitting.\n",
    "\n",
    "---\n",
    "\n",
    "## **Splitting Criteria**\n",
    "\n",
    "To determine the best feature to split on, the decision tree algorithm evaluates the following measures:\n",
    "\n",
    "### **1. Gini Impurity (for Classification)**\n",
    "\n",
    "The **Gini Impurity** measures how often a randomly selected element would be incorrectly classified. The formula is:\n",
    "\n",
    "$$\n",
    "\\text{Gini}(D) = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( p_i \\) is the proportion of class \\( i \\) in the dataset.\n",
    "- \\( C \\) is the number of classes.\n",
    "\n",
    "The goal is to choose the feature that results in the lowest Gini impurity.\n",
    "\n",
    "### **2. Entropy and Information Gain (for Classification)**\n",
    "\n",
    "**Entropy** measures the uncertainty or disorder in a dataset. It is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Entropy}(D) = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( p_i \\) is the proportion of class \\( i \\) in the dataset.\n",
    "\n",
    "**Information Gain** is the reduction in entropy when a feature is used to split the dataset. It is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Information Gain} = \\text{Entropy}(D) - \\sum_{v \\in V} \\frac{|D_v|}{|D|} \\text{Entropy}(D_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( D_v \\) represents the subset of data where the feature \\( v \\) has a particular value.\n",
    "\n",
    "The feature that maximizes information gain is selected for the split.\n",
    "\n",
    "### **3. Mean Squared Error (MSE) (for Regression)**\n",
    "\n",
    "For regression tasks, the algorithm uses the **Mean Squared Error (MSE)** as the impurity measure:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(D) = \\frac{1}{|D|} \\sum_{i=1}^{|D|} (y_i - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the actual value of the target variable.\n",
    "- \\( \\hat{y} \\) is the predicted value (mean of the target values in the node).\n",
    "\n",
    "The algorithm tries to minimize the MSE when choosing the best split.\n",
    "\n",
    "---\n",
    "\n",
    "## **Building a Decision Tree**\n",
    "\n",
    "1. **Choose the Best Feature**: At each node, evaluate each feature and calculate the Gini Impurity, Entropy, or MSE for all possible splits.\n",
    "2. **Split the Data**: Based on the chosen feature, divide the dataset into subsets.\n",
    "3. **Repeat**: Recursively apply the process to each subset, building the tree from the top (root) down to the leaf nodes.\n",
    "4. **Stop Criteria**: The recursion ends when the data in a node is pure or when the tree reaches a predefined maximum depth.\n",
    "\n",
    "---\n",
    "\n",
    "## **Pruning**\n",
    "\n",
    "**Pruning** is the process of removing unnecessary branches from the decision tree to avoid overfitting. There are two types of pruning:\n",
    "\n",
    "### **1. Pre-Pruning (Early Stopping)**\n",
    "\n",
    "Pre-pruning involves stopping the tree-building process early before it becomes too complex. This can be done by limiting:\n",
    "- The maximum depth of the tree.\n",
    "- The minimum number of samples required to split a node.\n",
    "- The minimum improvement in the splitting criterion.\n",
    "\n",
    "### **2. Post-Pruning (Cost Complexity Pruning)**\n",
    "\n",
    "Post-pruning involves first building the full tree and then trimming branches that have little importance. This can be done by evaluating the performance of the tree on a validation set and removing branches that don't contribute much to improving accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of Decision Trees**\n",
    "\n",
    "- **Easy to understand and interpret**: Decision trees are easy to visualize and understand, even for non-experts.\n",
    "- **No need for feature scaling**: Decision trees do not require normalization or standardization of data.\n",
    "- **Handles both numerical and categorical data**: Decision trees can handle different types of features without the need for preprocessing.\n",
    "- **Non-linear relationships**: Decision trees can capture complex, non-linear relationships between features.\n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages of Decision Trees**\n",
    "\n",
    "- **Overfitting**: Decision trees can easily overfit, especially with deep trees that fit noise in the data.\n",
    "- **Instability**: Small changes in the data can lead to completely different tree structures.\n",
    "- **Greedy Algorithm**: The decision tree algorithm is greedy, making locally optimal splits without considering the global best solution.\n",
    "- **Bias towards dominant classes**: In imbalanced datasets, decision trees may favor the majority class.\n",
    "\n",
    "---\n",
    "\n",
    "## **Ensemble Methods**\n",
    "\n",
    "To overcome some of the limitations of decision trees, ensemble methods like **Random Forests** and **Gradient Boosting** are often used. These methods combine multiple decision trees to improve accuracy and reduce overfitting.\n",
    "\n",
    "- **Random Forest**: Builds multiple decision trees using different subsets of data and features, and combines their results to improve robustness and accuracy.\n",
    "- **Gradient Boosting**: Builds decision trees sequentially, where each tree corrects the errors of the previous tree.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications**\n",
    "\n",
    "Decision trees are widely used in a variety of fields, including:\n",
    "- **Classification**: Spam detection, fraud detection, medical diagnoses.\n",
    "- **Regression**: Predicting house prices, sales forecasting.\n",
    "- **Feature Selection**: Used in feature engineering for reducing the number of variables in predictive models.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example**\n",
    "\n",
    "Given a dataset with features like **age** and **income**, the decision tree algorithm would:\n",
    "\n",
    "- Start at the root node and ask a question like: \"Is age greater than 40?\"\n",
    "- Based on the answer, it would split the data into two branches (e.g., age <= 40 and age > 40).\n",
    "- Each branch would continue to split based on another feature (e.g., \"Is income > 50k?\").\n",
    "- Eventually, the tree would end at leaf nodes where a class label or regression value is assigned.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "Decision trees are intuitive models used for classification and regression tasks. They split data into subsets based on feature values, aiming to create a simple decision rule for each subset. While they are easy to interpret, they are prone to overfitting and require careful tuning and pruning. Ensemble methods like Random Forests and Gradient Boosting can be used to improve performance and robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dec](images/dec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity\n",
    "\n",
    "Gini Impurity is a measure used in decision tree algorithms to evaluate how \"impure\" or \"mixed\" a node is. It quantifies the likelihood of a random element being incorrectly classified if it is randomly labeled based on the distribution of labels in the dataset. Lower Gini Impurity means that the node is more \"pure,\" i.e., most of the data points belong to a single class.\n",
    "\n",
    "#### Formula for Gini Impurity:\n",
    "\n",
    "$$\n",
    "\\text{Gini Impurity} = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( p_i \\) is the probability of an element belonging to class \\( i \\),\n",
    "- \\( k \\) is the number of unique classes.\n",
    "\n",
    "#### Intuition:\n",
    "- **Low Gini Impurity**: A value close to 0 means that most of the items in the node belong to the same class (i.e., the node is \"pure\").\n",
    "- **High Gini Impurity**: A value close to 1 means that the items in the node are evenly distributed across multiple classes (i.e., the node is \"impure\").\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Suppose we have a dataset of 100 items with two classes: **Class A** and **Class B**. 80 items belong to Class A, and 20 items belong to Class B. \n",
    "\n",
    "The Gini Impurity for this dataset is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Gini Impurity} = 1 - \\left( \\left( \\frac{80}{100} \\right)^2 + \\left( \\frac{20}{100} \\right)^2 \\right)\n",
    "$$\n",
    "$$\n",
    "\\text{Gini Impurity} = 1 - \\left( 0.64 + 0.04 \\right) = 1 - 0.68 = 0.32\n",
    "$$\n",
    "\n",
    "This means the impurity is 0.32, indicating that while Class A is dominant, there is still some mix of Class B in the dataset.\n",
    "\n",
    "#### Use in Decision Trees:\n",
    "\n",
    "In decision tree algorithms (e.g., CART), the goal is to **minimize the Gini Impurity** at each decision node. The algorithm evaluates different feature splits and selects the one that results in the lowest Gini Impurity in the child nodes, aiming to create nodes that are as pure as possible (i.e., contain mostly data points from a single class).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Using Gini Index\n",
    "\n",
    "The Gini Index (or Gini Impurity) is a key metric in decision tree algorithms (e.g., CART, Classification and Regression Trees) for building classification models. It is used to evaluate the quality of splits at each node of the tree and helps the algorithm decide which feature and threshold should be used to split the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. calculate gini index for whole dataset (label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gini](images/gini1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gini](images/gini3.png)\n",
    "![gini](images/gini4.png)\n",
    "![gini](images/gini5.png)\n",
    "![gini](images/gini6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Parameters Explained\n",
    "\n",
    "### 1. **`criterion`** (default=`'gini'`)\n",
    "This parameter specifies the function used to measure the **quality of a split** at each node.\n",
    "\n",
    "- **`'gini'`**: Measures **Gini impurity**. A lower Gini value indicates that the node contains mostly one class.\n",
    "- **`'entropy'`**: Measures **entropy** (information gain). This is based on information theory and maximizes the reduction in entropy.\n",
    "\n",
    "### 2. **`max_depth`** (default=`None`)\n",
    "The **maximum depth** of the tree. If set to `None`, the tree will grow until all nodes are pure or other stopping criteria are met.\n",
    "\n",
    "- Limiting the depth helps prevent **overfitting** by controlling the complexity of the tree.\n",
    "\n",
    "### 3. **`min_samples_split`** (default=`2`)\n",
    "This parameter specifies the **minimum number of samples** required to split an internal node.\n",
    "\n",
    "- **Default value is 2**, meaning any node with 2 or more samples can be split.\n",
    "- A higher value helps prevent **overfitting** by avoiding splits with fewer samples.\n",
    "\n",
    "### 4. **`min_samples_leaf`** (default=`1`)\n",
    "The **minimum number of samples** required to be at a leaf node.\n",
    "\n",
    "- **Default value is 1**, meaning a leaf can contain just one sample.\n",
    "- Increasing this value forces the tree to have more samples per leaf, reducing overfitting.\n",
    "\n",
    "### 5. **`max_features`** (default=`None`)\n",
    "Limits the number of features to consider when splitting a node.\n",
    "\n",
    "- **If set to `None`**, all features are used at each node.\n",
    "- **If set to an integer**, only that number of features are considered.\n",
    "- **If set to a float**, it uses a percentage of total features (e.g., `max_features=0.5` uses 50% of the features).\n",
    "\n",
    "### 6. **`max_leaf_nodes`** (default=`None`)\n",
    "Limits the number of **leaf nodes** in the tree.\n",
    "\n",
    "- If set to a positive integer, the tree will stop growing once it reaches that number of leaf nodes.\n",
    "- **If set to `None`**, the tree grows until other stopping criteria are met.\n",
    "\n",
    "### 7. **`min_impurity_decrease`** (default=`0.0`)\n",
    "The **minimum decrease in impurity** required for a node to split.\n",
    "\n",
    "- If the decrease in impurity is smaller than this value, the node will not split, helping to **prune** the tree.\n",
    "\n",
    "### 8. **`class_weight`** (default=`None`)\n",
    "Assigns different **weights to each class** in the dataset.\n",
    "\n",
    "- **`None`** means all classes are weighted equally.\n",
    "- **`\"balanced\"`** automatically adjusts weights inversely proportional to class frequencies.\n",
    "- You can manually set weights as a dictionary, e.g., `{0: 1, 1: 2}`.\n",
    "\n",
    "### 9. **`random_state`** (default=`None`)\n",
    "Controls the **random number generator** for reproducibility.\n",
    "\n",
    "- Ensures that the results can be reproduced if the same random seed is used.\n",
    "\n",
    "### 10. **`splitter`** (default=`'best'`)\n",
    "The strategy used to split a node.\n",
    "\n",
    "- **`'best'`**: Selects the best split based on the chosen criterion.\n",
    "- **`'random'`**: Randomly selects a split, useful in ensemble methods like Random Forests.\n",
    "\n",
    "### 11. **`presort`** (default=`False`)\n",
    "This parameter is deprecated in newer versions of `sklearn`.\n",
    "\n",
    "- It was previously used to **pre-sort** the data before splitting, but is now handled automatically.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Putting It All Together\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='entropy',    # Use 'entropy' to calculate splits based on information gain\n",
    "    max_depth=5,            # Limit the tree depth to avoid overfitting\n",
    "    min_samples_split=4,    # A node must have at least 4 samples to be split\n",
    "    min_samples_leaf=2,     # A leaf must have at least 2 samples\n",
    "    max_features='sqrt',    # Use only a random subset of features (e.g., sqrt of total features)\n",
    "    random_state=42         # Ensure the results are reproducible\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
