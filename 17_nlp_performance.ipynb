{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics for NLP Models\n",
    "\n",
    "In Natural Language Processing (NLP), the performance of models can be evaluated using several different metrics, depending on the task at hand (e.g., classification, sequence tagging, generation, etc.). Below is a breakdown of the most commonly used evaluation metrics in NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Accuracy**\n",
    "\n",
    "### Definition:\n",
    "Accuracy measures the percentage of correct predictions made by the model, typically used for classification tasks.\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} = \\frac{\\sum_{i=1}^{N} I(y_i = \\hat{y}_i)}{N}\n",
    "$$\n",
    "Where:\n",
    "- \\( y_i \\) = true label (e.g., actual class or category)\n",
    "- \\( \\hat{y}_i \\) = predicted label\n",
    "- \\( I \\) = indicator function (1 if correct, 0 otherwise)\n",
    "- \\( N \\) = number of samples\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Precision, Recall, and F1-Score**\n",
    "\n",
    "These metrics are especially useful for tasks where the class distribution is imbalanced (e.g., in sentiment analysis, spam classification).\n",
    "\n",
    "### Precision:\n",
    "Measures how many of the predicted positive instances are actually positive.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "Where:\n",
    "- \\( TP \\) = True Positives (correct positive predictions)\n",
    "- \\( FP \\) = False Positives (incorrect positive predictions)\n",
    "\n",
    "### Recall (Sensitivity):\n",
    "Measures how many of the actual positive instances were correctly predicted.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "Where:\n",
    "- \\( FN \\) = False Negatives (incorrectly predicted as negative)\n",
    "\n",
    "### F1-Score:\n",
    "The harmonic mean of precision and recall, giving a balance between the two.\n",
    "\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Macro and Micro Averaging (for Multi-Class Classification)**\n",
    "\n",
    "When evaluating models on multi-class tasks, metrics like precision, recall, and F1 can be averaged over classes. Two common types of averaging are:\n",
    "\n",
    "### Micro Averaging:\n",
    "This calculates the metrics globally by counting the total true positives, false positives, etc., across all classes.\n",
    "$$\n",
    "\\text{Micro Precision} = \\frac{\\sum_{i=1}^{C} TP_i}{\\sum_{i=1}^{C} (TP_i + FP_i)}\n",
    "$$\n",
    "\n",
    "### Macro Averaging:\n",
    "This computes metrics for each class individually and then takes the average. Each class is given equal weight regardless of its size.\n",
    "$$\n",
    "\\text{Macro Precision} = \\frac{1}{C} \\sum_{i=1}^{C} \\frac{TP_i}{TP_i + FP_i}\n",
    "$$\n",
    "Where \\( C \\) is the number of classes.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. BLEU Score (for Machine Translation)**\n",
    "\n",
    "### Definition:\n",
    "The BLEU (Bilingual Evaluation Understudy) score is a metric for evaluating the quality of machine-generated text, particularly in machine translation tasks. It compares the n-grams of the predicted text with those in the reference text.\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "\\text{BLEU} = \\text{BP} \\times \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)\n",
    "$$\n",
    "Where:\n",
    "- \\( BP \\) = Brevity Penalty, which penalizes short translations.\n",
    "- \\( p_n \\) = Precision for n-grams (n=1, 2, 3, 4).\n",
    "- \\( w_n \\) = Weight for each n-gram.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Perplexity (for Language Models)**\n",
    "\n",
    "### Definition:\n",
    "Perplexity is a measure of how well a probability model predicts a sample. In NLP, it is commonly used to evaluate language models, with lower values indicating better performance.\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "\\text{Perplexity} = 2^{H(p)}\n",
    "$$\n",
    "Where:\n",
    "- \\( H(p) \\) = Cross-entropy between the true distribution and the predicted distribution.\n",
    "- Lower perplexity values indicate that the model has a better ability to predict the next word.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. ROUGE Score (for Summarization)**\n",
    "\n",
    "### Definition:\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is used to evaluate automatic summarization and machine-generated text. It measures the overlap between n-grams, words, or word sequences in the predicted summary and the reference summary.\n",
    "\n",
    "### ROUGE-N (Recall):\n",
    "$$\n",
    "\\text{ROUGE-N} = \\frac{\\sum_{i=1}^{N} \\text{Count of common n-grams}}{\\sum_{i=1}^{N} \\text{Count of n-grams in the reference summary}}\n",
    "$$\n",
    "\n",
    "### ROUGE-L (Longest Common Subsequence):\n",
    "$$\n",
    "\\text{ROUGE-L} = \\frac{\\sum_{i=1}^{N} LCS(P, R)}{\\sum_{i=1}^{N} |R|}\n",
    "$$\n",
    "Where:\n",
    "- \\( LCS(P, R) \\) is the length of the longest common subsequence between the predicted (P) and reference (R) summaries.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Word Error Rate (WER) (for Speech Recognition)**\n",
    "\n",
    "### Definition:\n",
    "WER is a metric used to evaluate the performance of speech recognition systems. It calculates the difference between the predicted transcriptions and the reference transcriptions.\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "\\text{WER} = \\frac{S + D + I}{N}\n",
    "$$\n",
    "Where:\n",
    "- \\( S \\) = Substitutions (incorrectly recognized words)\n",
    "- \\( D \\) = Deletions (missing words)\n",
    "- \\( I \\) = Insertions (extra words)\n",
    "- \\( N \\) = Total number of words in the reference\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Kendall’s Tau and Spearman’s Rank Correlation (for Ranking Tasks)**\n",
    "\n",
    "### Definition:\n",
    "Kendall’s Tau and Spearman’s Rank Correlation are used to evaluate models in ranking tasks (e.g., recommendation systems). These metrics measure the correlation between the predicted ranking and the true ranking.\n",
    "\n",
    "### Kendall’s Tau:\n",
    "$$\n",
    "\\tau = \\frac{C - D}{\\sqrt{(C + D + T) \\cdot (C + D + U)}}\n",
    "$$\n",
    "Where:\n",
    "- \\( C \\) = number of concordant pairs\n",
    "- \\( D \\) = number of discordant pairs\n",
    "- \\( T, U \\) = tied pairs for prediction and actual rankings\n",
    "\n",
    "### Spearman’s Rank Correlation:\n",
    "$$\n",
    "\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\n",
    "$$\n",
    "Where:\n",
    "- \\( d_i \\) = difference between ranks of each item\n",
    "- \\( n \\) = number of items\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Cosine Similarity (for Text Similarity)**\n",
    "\n",
    "### Definition:\n",
    "Cosine similarity is used to measure the similarity between two non-zero vectors, which can represent word embeddings or document vectors. It is commonly used in information retrieval and document similarity tasks.\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "$$\n",
    "Where:\n",
    "- \\( A \\) and \\( B \\) are vectors representing two documents (or words).\n",
    "- \\( \\|A\\| \\) and \\( \\|B\\| \\) are the magnitudes of the vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Normalized Discounted Cumulative Gain (NDCG) (for Ranking)**\n",
    "\n",
    "### Definition:\n",
    "NDCG is used for evaluating the quality of ranked retrieval results, such as in search engines. It considers the position of relevant documents in the ranking.\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "\\text{NDCG@k} = \\frac{Z_k}{\\sum_{i=1}^{k} \\frac{rel(i)}{\\log_2(i + 1)}}\n",
    "$$\n",
    "Where:\n",
    "- \\( Z_k \\) is a normalization factor to make the score range from 0 to 1.\n",
    "- \\( rel(i) \\) is the relevance of the item at position \\( i \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "These metrics are critical in evaluating NLP models, particularly for tasks like classification, sequence labeling, machine translation, and summarization. The choice of metric depends on the specific task and the nature of the data. By understanding these metrics, you can better assess your model's performance, diagnose issues, and improve the overall quality of your NLP systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
