{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression is a technique used to predict a continuous output variable based on one or more input features. It assumes a linear relationship between the input variables (also known as features) and the output (target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: A Comprehensive Guide\n",
    "\n",
    "Linear Regression is one of the most fundamental algorithms in machine learning, used for predicting continuous values based on input features. It assumes a linear relationship between the dependent variable (target) and one or more independent variables (features).\n",
    "\n",
    "## 1. Simple Linear Regression (Single Feature)\n",
    "\n",
    "In **simple linear regression**, we predict a continuous output variable \\( y \\) based on a single feature \\( x \\).\n",
    "\n",
    "### Equation for Simple Linear Regression:\n",
    "$$\n",
    "y = mx + b\n",
    "$$\n",
    "Where:\n",
    "- \\( y \\) is the predicted value (output).\n",
    "- \\( x \\) is the input feature (independent variable).\n",
    "- \\( m \\) is the **slope** (coefficient), representing the change in \\( y \\) for a one-unit change in \\( x \\).\n",
    "- \\( b \\) is the **intercept**, the value of \\( y \\) when \\( x = 0 \\).\n",
    "\n",
    "### Goal:\n",
    "The goal is to find the values of \\( m \\) (slope) and \\( b \\) (intercept) that minimize the error between the predicted and actual values of \\( y \\).\n",
    "\n",
    "#### Steps to Fit the Line:\n",
    "1. **Collect Data**: Dataset consisting of pairs \\( (x, y) \\).\n",
    "2. **Calculate the coefficients \\( m \\) and \\( b \\)**:\n",
    "   - **Slope (m)**:\n",
    "   $$\n",
    "   m = \\frac{n \\sum{xy} - \\sum{x} \\sum{y}}{n \\sum{x^2} - (\\sum{x})^2}\n",
    "   $$\n",
    "   - **Intercept (b)**:\n",
    "   $$\n",
    "   b = \\frac{\\sum{y} - m \\sum{x}}{n}\n",
    "   $$\n",
    "   Where \\( n \\) is the number of data points.\n",
    "3. **Make Predictions**: Use the calculated coefficients to predict \\( y \\) for any given \\( x \\).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Multiple Linear Regression (Multiple Features)\n",
    "\n",
    "**Multiple linear regression** extends simple linear regression to handle multiple input features.\n",
    "\n",
    "### Equation for Multiple Linear Regression:\n",
    "$$\n",
    "y = b + m_1x_1 + m_2x_2 + \\dots + m_nx_n\n",
    "$$\n",
    "Where:\n",
    "- \\( y \\) is the predicted value.\n",
    "- \\( $x_1$, $x_2$,.., $x_n$ \\) are the independent features.\n",
    "- \\( $m_1$, $m_2$,.., $m_n$ \\) are the coefficients (slopes) for each feature.\n",
    "- \\( b \\) is the intercept.\n",
    "\n",
    "### Goal:\n",
    "The goal is to find the coefficients \\( $m_1$,$m_2$, \\dots, $m_n$ \\) and the intercept \\( b \\) that minimize the error between predicted and actual \\( y \\) values.\n",
    "\n",
    "#### Steps to Fit the Model:\n",
    "1. **Matrix Formulation**:\n",
    "   $$\n",
    "   Y = X \\beta + \\epsilon\n",
    "   $$\n",
    "   Where:\n",
    "   - \\( Y \\) is the vector of observed values.\n",
    "   - \\( X \\) is the matrix of input features.\n",
    "   - \\( \\beta \\) is the vector of coefficients.\n",
    "   - \\( \\epsilon \\) is the error term.\n",
    "\n",
    "2. **Normal Equation**:\n",
    "   The optimal coefficients \\( \\beta \\) can be calculated using the normal equation:\n",
    "   $$\n",
    "   \\beta = (X^T X)^{-1} X^T Y\n",
    "   $$\n",
    "   Where \\( X^T \\) is the transpose of the feature matrix, and \\( (X^T X)^{-1} \\) is its inverse.\n",
    "\n",
    "3. **Prediction**: \n",
    "   After calculating \\( \\beta \\), we make predictions using:\n",
    "   $$\n",
    "   \\hat{Y} = X \\beta\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Cost Function (Mean Squared Error)\n",
    "\n",
    "The **cost function** measures how well the model fits the data. The most common cost function is **Mean Squared Error (MSE)**, which computes the average squared difference between actual and predicted values.\n",
    "\n",
    "### Formula for MSE:\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "Where:\n",
    "- \\( n \\) is the number of data points.\n",
    "- \\( y_i \\) is the actual value.\n",
    "- \\( \\hat{y_i} \\) is the predicted value.\n",
    "\n",
    "### Goal:\n",
    "Minimize the MSE to improve the model’s performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Model Evaluation\n",
    "\n",
    "After fitting the linear regression model, we use several metrics to evaluate its performance:\n",
    "\n",
    "### Key Evaluation Metrics:\n",
    "- **R-squared (R²)**: Measures the proportion of variance in the dependent variable that is predictable from the independent variables. \\( R^2 \\) ranges from 0 to 1, with 1 indicating a perfect fit.\n",
    "  \n",
    "  **Formula for \\( R^2 \\)**:\n",
    "  $$\n",
    "  R^2 = 1 - \\frac{\\sum (y_i - \\hat{y_i})^2}{\\sum (y_i - \\bar{y})^2}\n",
    "  $$\n",
    "  Where \\( \\bar{y} \\) is the mean of the actual values.\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**: Measures the square root of the average squared errors, representing the model's average prediction error in the same units as the target variable.\n",
    "  \n",
    "  **Formula for RMSE**:\n",
    "  $$\n",
    "  RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}\n",
    "  $$\n",
    "\n",
    "### Goal:\n",
    "- High **R²** values and low **MSE** or **RMSE** indicate a good model fit.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Assumptions of Linear Regression\n",
    "\n",
    "Linear regression makes several assumptions about the data:\n",
    "1. **Linearity**: There is a linear relationship between the dependent and independent variables.\n",
    "2. **Independence**: Observations are independent of each other.\n",
    "3. **Homoscedasticity**: The variance of errors is constant across all levels of the independent variable(s).\n",
    "4. **Normality**: The residuals (errors) are normally distributed.\n",
    "\n",
    "Violations of these assumptions may lead to biased predictions or inefficiencies in the model.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Example Code: Simple Linear Regression in Python (Using Scikit-learn)\n",
    "\n",
    "Here's an example of how to implement **Simple Linear Regression** using **Scikit-learn** in Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10  # Random input feature between 0 and 10\n",
    "y = 2.5 * X + np.random.randn(100, 1) * 2  # Linear relation with some noise\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual data')\n",
    "plt.plot(X_test, y_pred, color='red', label='Fitted line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
