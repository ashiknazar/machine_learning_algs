{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Algorithm\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the loss function in machine learning and deep learning. It iteratively adjusts the parameters (such as weights in a model) to reduce the error (or loss) between predicted outputs and actual results. It is one of the most widely used techniques for training models.\n",
    "\n",
    "## How Gradient Descent Works\n",
    "\n",
    "1. **Objective (Loss) Function:**\n",
    "   Gradient Descent is used to minimize a **loss function** (also known as the cost function or objective function). The loss function measures the difference between the predicted and actual outputs. In a regression model, for instance, this could be the Mean Squared Error (MSE).\n",
    "\n",
    "2. **Gradient Computation:**\n",
    "   The **gradient** is a vector that represents the direction of the greatest increase in the loss function. The gradient is computed for the model parameters (e.g., weights), indicating how much the loss function will change if the parameters are adjusted.\n",
    "\n",
    "3. **Parameter Update:**\n",
    "   Once the gradient is calculated, the model's parameters are updated in the opposite direction of the gradient to minimize the loss function. The size of each step is determined by the **learning rate**.\n",
    "\n",
    "   The parameter update rule is:\n",
    "   $$\n",
    "   \\theta = \\theta - \\alpha \\cdot \\nabla_{\\theta} J(\\theta)\n",
    "   $$\n",
    "   where:\n",
    "   - $\\theta$ is the model parameter (e.g., weight),\n",
    "   - $\\alpha$ is the learning rate,\n",
    "   - $\\nabla_{\\theta} J(\\theta)$ is the gradient of the loss function $J(\\theta)$ with respect to $\\theta$.\n",
    "\n",
    "4. **Repeat:**\n",
    "   The process of calculating the gradient and updating the parameters is repeated for a predefined number of iterations or until the algorithm converges to a minimum.\n",
    "\n",
    "## Types of Gradient Descent\n",
    "\n",
    "There are three main types of Gradient Descent, each varying in the way the gradient is calculated and the parameters are updated:\n",
    "\n",
    "### 1. **Batch Gradient Descent (BGD):**\n",
    "   - In **batch gradient descent**, the gradient is calculated using the entire dataset. This approach ensures stable and smooth convergence but can be computationally expensive for large datasets.\n",
    "\n",
    "### 2. **Stochastic Gradient Descent (SGD):**\n",
    "   - In **stochastic gradient descent**, the gradient is calculated using a single data point at each iteration. This speeds up the learning process but introduces high variance, causing more oscillations in the loss function curve.\n",
    "\n",
    "### 3. **Mini-Batch Gradient Descent:**\n",
    "   - **Mini-batch gradient descent** is a compromise between batch and stochastic gradient descent. It uses a small, random subset (mini-batch) of the data to compute the gradient. This approach balances the efficiency of batch gradient descent with the speed of stochastic gradient descent.\n",
    "\n",
    "## Key Parameters in Gradient Descent\n",
    "\n",
    "### 1. **Learning Rate ($\\alpha$):**\n",
    "   - The learning rate controls the size of the steps taken in the direction of the gradient. If the learning rate is too small, convergence will be slow. If it's too large, the algorithm may overshoot the optimal solution.\n",
    "\n",
    "### 2. **Convergence:**\n",
    "   - The algorithm is considered to have converged when the change in the loss function or in the parameters becomes very small, indicating that the model has found optimal or near-optimal parameters.\n",
    "\n",
    "## Mathematical Intuition Behind Gradient Descent\n",
    "\n",
    "In the simplest case of **linear regression**, the objective function is the **Mean Squared Error (MSE)** between the predicted and actual values:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "$$\n",
    "Where:\n",
    "- $J(\\theta)$ is the cost function,\n",
    "- $h_\\theta(x^{(i)})$ is the model's prediction for input $x^{(i)}$,\n",
    "- $y^{(i)}$ is the actual output for the $i$-th example,\n",
    "- $m$ is the total number of training examples.\n",
    "\n",
    "The gradient of the cost function $J(\\theta)$ with respect to the parameters $\\theta$ (weights) is:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x^{(i)}\n",
    "$$\n",
    "\n",
    "Using this gradient, the parameters are updated as:\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\cdot \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "## Applications of Gradient Descent\n",
    "\n",
    "Gradient Descent is used extensively in machine learning and deep learning algorithms, including:\n",
    "- **Linear Regression**: To minimize the cost function and find the optimal weights.\n",
    "- **Logistic Regression**: To optimize the parameters for binary classification.\n",
    "- **Neural Networks**: To adjust the weights and biases of the network using backpropagation and gradient descent.\n",
    "- **Support Vector Machines (SVM)**: To find the optimal hyperplane for classification tasks.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Gradient Descent is a powerful and versatile optimization algorithm that allows machine learning models to find the optimal parameters by iteratively minimizing a loss function. By understanding and choosing the right type of gradient descent and tuning its parameters, we can train models effectively and efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in linear regression - > we optimize intercept and slop\n",
    "- in logistic regressio -> we optimize squiggle\n",
    "- in tsne we optimize clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- in linear regression for example\n",
    "  - we  will start by using gradient descent to find intercept\n",
    "  - then once we understand how gradient descent works ,we will use it to solve for the intercept and slope\n",
    "  - eg: for slope 0.64 find optimal value for intercept\n",
    "  - first pick a random value for intercept\n",
    "  - calculate loss\n",
    "  - plot different intercepts on x and curresponding loss on y axis \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/grad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select point with lowest loss\n",
    "-   gradient descent is more efficient , it only does a few calculations far from the optimal solution and increases the number of calculations closer to optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/gr_eff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we get a equation for curve\n",
    "- calculating derivative of function at any point will give slope\n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
