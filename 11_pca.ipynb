{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Principal Component Analysis (PCA)**\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction while preserving as much variability as possible. PCA transforms high-dimensional data into a lower-dimensional form, making it easier to analyze and visualize without losing significant information. It is widely used in exploratory data analysis, feature extraction, and noise reduction.\n",
    "\n",
    "PCA finds new features (called **principal components**) that are linear combinations of the original features. These new components are ordered by the amount of variance they capture from the data, with the first component capturing the most variance, the second component capturing the second most variance, and so on.\n",
    "\n",
    "---\n",
    "\n",
    "## **How PCA Works**\n",
    "\n",
    "PCA works by performing the following steps:\n",
    "\n",
    "1. **Standardize the Data**: \n",
    "   - Since PCA is affected by the scale of the features, it is essential to standardize the data so that each feature has zero mean and unit variance. Standardization is typically done by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "   $$ X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma} $$\n",
    "\n",
    "2. **Compute the Covariance Matrix**: \n",
    "   - The covariance matrix captures the relationships between different features. Each element in the covariance matrix represents the covariance between two features.\n",
    "   - If the features are highly correlated, the covariance will be large, indicating that one feature can be predicted from the other.\n",
    "\n",
    "   $$ \\text{Cov}(X) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu)(x_i - \\mu)^T $$\n",
    "\n",
    "3. **Compute Eigenvalues and Eigenvectors**: \n",
    "   - The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues represent the magnitude of the variance along these directions.\n",
    "   - The eigenvectors are orthogonal (perpendicular) to each other.\n",
    "\n",
    "4. **Sort Eigenvalues and Eigenvectors**:\n",
    "   - Sort the eigenvectors by the corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues represent the directions with the most variance in the data.\n",
    "\n",
    "5. **Choose the Top k Principal Components**:\n",
    "   - Select the top k eigenvectors (principal components) that correspond to the largest eigenvalues. These principal components will form a new feature space with reduced dimensionality.\n",
    "\n",
    "6. **Transform the Data**:\n",
    "   - Multiply the standardized data by the top k eigenvectors to obtain the transformed data in the lower-dimensional space.\n",
    "\n",
    "   $$ X_{\\text{transformed}} = X_{\\text{standardized}} \\cdot V_k $$\n",
    "\n",
    "   Where \\( V_k \\) is the matrix of the top k eigenvectors.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "Given a dataset with \\( n \\) samples and \\( p \\) features, we want to find a transformation that reduces the number of features from \\( p \\) to \\( k \\) (where \\( k < p \\)).\n",
    "\n",
    "### **Steps in Detail**:\n",
    "\n",
    "1. **Standardize the dataset**:\n",
    "   For each feature \\( X_j \\), subtract the mean \\( \\mu_j \\) and divide by the standard deviation \\( \\sigma_j \\):\n",
    "   $$ X_{\\text{standardized}} = \\frac{X_j - \\mu_j}{\\sigma_j} $$\n",
    "\n",
    "2. **Compute the covariance matrix**:\n",
    "   $$ \\text{Cov}(X) = \\frac{1}{n} X_{\\text{standardized}}^T X_{\\text{standardized}} $$\n",
    "\n",
    "3. **Eigenvalue Decomposition**:\n",
    "   Solve the equation:\n",
    "   $$ \\text{Cov}(X) \\cdot v_i = \\lambda_i \\cdot v_i $$\n",
    "\n",
    "   Where:\n",
    "   - \\( \\lambda_i \\) is the eigenvalue associated with the \\( i^{th} \\) eigenvector \\( v_i \\).\n",
    "\n",
    "4. **Sort eigenvalues and eigenvectors**:\n",
    "   - Sort the eigenvalues \\( \\lambda_i \\) in descending order and choose the corresponding eigenvectors \\( v_i \\) with the largest eigenvalues.\n",
    "\n",
    "5. **Form the feature matrix**:\n",
    "   - Construct a matrix \\( V_k \\) with the top \\( k \\) eigenvectors as columns.\n",
    "\n",
    "6. **Transform the data**:\n",
    "   - Multiply the standardized data by the matrix \\( V_k \\) to obtain the reduced dimensionality representation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Choosing the Number of Components (k)**\n",
    "\n",
    "The number of principal components, \\( k \\), is often chosen by analyzing the **explained variance ratio**. This ratio indicates how much of the variance in the data is captured by each principal component.\n",
    "\n",
    "- **Cumulative explained variance**: Sum the explained variance ratios of the first \\( k \\) components to decide how many components to keep.\n",
    "- **Scree plot**: A plot of the eigenvalues or explained variance ratio can help determine the optimal number of components by looking for an \"elbow\" point where the variance explained by additional components starts to level off.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of PCA**\n",
    "\n",
    "- **Reduces dimensionality**: PCA reduces the number of features while retaining most of the original variance, making the data easier to visualize and analyze.\n",
    "- **Improves computational efficiency**: Working with fewer features can speed up downstream algorithms, especially when dealing with high-dimensional data.\n",
    "- **Eliminates correlations between features**: By creating orthogonal principal components, PCA removes multicollinearity in the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages of PCA**\n",
    "\n",
    "- **Loss of interpretability**: The principal components are linear combinations of the original features, making it difficult to interpret the transformed features in terms of the original features.\n",
    "- **Sensitive to scaling**: PCA is sensitive to the scale of the features, which is why standardization is a critical step.\n",
    "- **Linear transformation**: PCA assumes linear relationships between features, which might not capture complex, nonlinear patterns in the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications of PCA**\n",
    "\n",
    "- **Data visualization**: PCA is often used to reduce the dimensions of the data for visualization (e.g., projecting data onto the first two principal components for a 2D plot).\n",
    "- **Noise reduction**: By discarding low-variance components, PCA can help remove noise from the data.\n",
    "- **Feature extraction**: In machine learning, PCA can be used to generate new features that retain most of the variance, making the model more efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example of PCA in Python**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Plot the transformed data\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.show()\n",
    "\n",
    "# Explained variance ratio\n",
    "print(f'Explained variance ratio: {pca.explained_variance_ratio_}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal Component Analysis (PCA) is a technique used in statistics and machine learning to simplify complex datasets by reducing their dimensions while retaining as much variance (information) as possible. It's often used for data compression, visualization, noise reduction, and feature extraction. Let me walk you through the key concepts and steps involved in PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>High-Dimensional Data:</b> You might have a dataset with many features (variables), which can be difficult to visualize, understand, or process. PCA helps reduce the number of features by transforming the data into a smaller set of uncorrelated features called principal components (PCs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Variance:</b> PCA tries to capture the maximum variance (spread) in the data. This means the first principal component captures the direction in the dataset where the data varies the most, the second principal component captures the next most important direction (orthogonal to the first), and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Linear Transformation:</b> PCA involves finding a new coordinate system for the data. Each axis in this new coordinate system is a principal component, which is a linear combination of the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Standardize the Data\n",
    "2. Calculate the Covariance Matrix\n",
    "3. Compute the Eigenvalues and Eigenvectors of the Covariance Matrix\n",
    "4. Sort the Eigenvalues and Eigenvectors\n",
    "5. Choose the Number of Principal Components\n",
    "6. Transform the Data\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
