{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms Overview\n",
    "\n",
    "Machine learning algorithms are broadly categorized into **Supervised Learning** and **Unsupervised Learning**. Below is a breakdown of key algorithms in both categories:\n",
    "\n",
    "## Supervised Learning\n",
    "In supervised learning, the algorithm is trained on labeled data (input-output pairs), with the goal of predicting the output for new, unseen data.\n",
    "\n",
    "### 1. **Linear Regression** (for regression tasks)\n",
    "- **Problem**: Predicting a continuous output variable.\n",
    "- **How it works**: Fits a linear relationship between input features and the target variable.\n",
    "- **Equation**: \\( y = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b \\)\n",
    "- **Key concept**: Minimizing error by finding the best weights.\n",
    "\n",
    "### 2. **Logistic Regression** (for binary classification tasks)\n",
    "- **Problem**: Classifying into one of two classes.\n",
    "- **How it works**: Uses the logistic (sigmoid) function to model probabilities of class membership.\n",
    "- **Equation**: \\( p(y=1|x) = \\frac{1}{1 + e^{-(w_1x_1 + w_2x_2 + \\dots + w_nx_n + b)}} \\)\n",
    "- **Key concept**: Predicts the probability of class 1 and classifies based on a threshold (typically 0.5).\n",
    "\n",
    "### 3. **K-Nearest Neighbors (KNN)** (for classification and regression)\n",
    "- **Problem**: Classifying or predicting based on the proximity of data points.\n",
    "- **How it works**: Assigns class or value based on the majority (classification) or average (regression) of the K nearest neighbors.\n",
    "- **Key concept**: Uses distance metrics (like Euclidean distance) to find nearest neighbors.\n",
    "\n",
    "### 4. **Support Vector Machines (SVM)** (for classification tasks)\n",
    "- **Problem**: Finding a hyperplane that separates data into different classes.\n",
    "- **How it works**: Maximizes the margin between classes while minimizing classification error.\n",
    "- **Key concept**: Uses support vectors (closest points) to define the optimal hyperplane.\n",
    "\n",
    "### 5. **Decision Trees** (for classification and regression)\n",
    "- **Problem**: Making decisions based on feature values.\n",
    "- **How it works**: Recursively splits data at decision nodes based on feature thresholds.\n",
    "- **Key concept**: Uses measures like **Gini impurity** or **information gain** (for classification) or **variance reduction** (for regression).\n",
    "\n",
    "### 6. **Random Forests** (for classification and regression)\n",
    "- **Problem**: Improving decision trees by combining multiple trees.\n",
    "- **How it works**: Builds an ensemble of decision trees using bootstrapped subsets of data and features. Predictions are made by aggregating results from all trees.\n",
    "- **Key concept**: **Bagging** technique reduces overfitting and improves performance.\n",
    "\n",
    "### 7. **Gradient Boosting Machines (GBM)** / **XGBoost** (for classification and regression)\n",
    "- **Problem**: Boosting weak models to create a stronger model.\n",
    "- **How it works**: Builds trees sequentially, each one correcting errors from the previous tree.\n",
    "- **Key concept**: **Gradient descent** is used to minimize the residual errors between trees.\n",
    "\n",
    "### 8. **Neural Networks** (for complex tasks like image recognition, NLP)\n",
    "- **Problem**: Learning complex patterns from high-dimensional data.\n",
    "- **How it works**: Composed of layers of interconnected neurons, each applying a weighted sum followed by an activation function.\n",
    "- **Key concept**: **Backpropagation** optimizes the weights through gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## Unsupervised Learning\n",
    "In unsupervised learning, the algorithm works with data that has no labeled outputs. The goal is to discover hidden patterns or structure in the data.\n",
    "\n",
    "### 1. **K-Means Clustering**\n",
    "- **Problem**: Grouping data points into K clusters.\n",
    "- **How it works**: Partitions data into K clusters, where each point is assigned to the nearest centroid. The centroids are recalculated iteratively until convergence.\n",
    "- **Key concept**: Uses **Euclidean distance** to measure similarity between points.\n",
    "\n",
    "### 2. **Hierarchical Clustering**\n",
    "- **Problem**: Creating a tree-like structure (dendrogram) of clusters.\n",
    "- **How it works**: Either starts with each point as its own cluster and merges the closest pairs (agglomerative), or starts with one large cluster and splits it (divisive).\n",
    "- **Key concept**: Can create clusters of varying shapes and sizes, based on similarity or distance.\n",
    "\n",
    "### 3. **Principal Component Analysis (PCA)**\n",
    "- **Problem**: Reducing the dimensionality of the data while retaining most of the variance.\n",
    "- **How it works**: Identifies principal components (directions of maximum variance) and projects the data into a lower-dimensional space.\n",
    "- **Key concept**: Used for **feature extraction** and **data visualization**.\n",
    "\n",
    "### 4. **Independent Component Analysis (ICA)**\n",
    "- **Problem**: Separating mixed signals into independent components.\n",
    "- **How it works**: Similar to PCA but aims to find statistically independent components rather than orthogonal components.\n",
    "- **Key concept**: Often used in signal processing (e.g., separating audio signals).\n",
    "\n",
    "### 5. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n",
    "- **Problem**: Identifying clusters of arbitrary shapes and handling noise in data.\n",
    "- **How it works**: Groups together closely packed points and labels points in sparse regions as noise.\n",
    "- **Key concept**: Does not require the number of clusters to be specified beforehand, unlike K-means.\n",
    "\n",
    "### 6. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
    "- **Problem**: Reducing high-dimensional data to 2 or 3 dimensions for visualization.\n",
    "- **How it works**: Maps similar points from high-dimensional space to nearby points in lower-dimensional space, preserving local structure.\n",
    "- **Key concept**: Often used for **data visualization** in high-dimensional datasets.\n",
    "\n",
    "### 7. **Autoencoders** (for dimensionality reduction, feature learning)\n",
    "- **Problem**: Learning a compact representation of the data.\n",
    "- **How it works**: Consists of an encoder (compresses the input data) and a decoder (reconstructs the data). The goal is to minimize reconstruction error.\n",
    "- **Key concept**: Used in **anomaly detection**, **image denoising**, and **data compression**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### **Supervised Learning Algorithms**:\n",
    "- **Linear Regression**: Predicts continuous output.\n",
    "- **Logistic Regression**: Predicts binary classification.\n",
    "- **K-Nearest Neighbors (KNN)**: Classifies or predicts based on nearest neighbors.\n",
    "- **Support Vector Machines (SVM)**: Finds optimal hyperplane to separate classes.\n",
    "- **Decision Trees**: Makes decisions based on feature values.\n",
    "- **Random Forests**: Aggregates decision trees to improve performance.\n",
    "- **Gradient Boosting (GBM, XGBoost)**: Builds strong predictive models by boosting weak learners.\n",
    "- **Neural Networks**: Learns complex patterns via multiple layers of neurons.\n",
    "\n",
    "### **Unsupervised Learning Algorithms**:\n",
    "- **K-Means**: Clusters data into K groups.\n",
    "- **Hierarchical Clustering**: Builds a hierarchical tree of clusters.\n",
    "- **PCA**: Reduces data dimensionality while preserving variance.\n",
    "- **ICA**: Separates mixed signals into independent components.\n",
    "- **DBSCAN**: Clusters data with varying shapes and handles noise.\n",
    "- **t-SNE**: Reduces high-dimensional data for visualization.\n",
    "- **Autoencoders**: Learns compressed representations of data.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to dive deeper into any specific algorithm or request code examples if needed!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
