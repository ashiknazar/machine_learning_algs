{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique primarily used for the visualization of high-dimensional datasets in a lower-dimensional space (typically 2D or 3D). It is particularly effective at preserving local relationships and is widely used for visualizing high-dimensional data in fields like machine learning, bioinformatics, and image processing.\n",
    "\n",
    "t-SNE works by converting similarities between data points into joint probabilities and aims to minimize the divergence between these probabilities in the higher and lower-dimensional spaces. It is a non-linear technique, meaning it does not assume any specific linear relationship between the dimensions of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **How t-SNE Works**\n",
    "\n",
    "The basic idea behind t-SNE is to:\n",
    "1. **Compute pairwise similarities** in the original high-dimensional space (using Gaussian distributions).\n",
    "2. **Project the data points into a lower-dimensional space** (2D or 3D) while trying to preserve the local structure, i.e., points that are similar in the high-dimensional space should remain close in the low-dimensional space, and dissimilar points should be farther apart.\n",
    "\n",
    "### **Steps of t-SNE**:\n",
    "1. **Compute pairwise similarities in the high-dimensional space**:\n",
    "   - The similarity between two points \\( x_i \\) and \\( x_j \\) is modeled as a conditional probability:\n",
    "   $$ p_{ij} = \\frac{\\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{\\|x_i - x_k\\|^2}{2\\sigma_i^2}\\right)} $$ \n",
    "   where \\( \\|x_i - x_j\\|^2 \\) is the squared Euclidean distance between the points \\( x_i \\) and \\( x_j \\), and \\( \\sigma_i \\) is a parameter controlling the spread of the Gaussian.\n",
    "\n",
    "2. **Convert the similarities to probabilities**:\n",
    "   - The joint probability \\( p_{ij} \\) is symmetric, meaning \\( p_{ij} = p_{ji} \\). The joint probability distribution reflects the likelihood that two points are neighbors in the original high-dimensional space.\n",
    "\n",
    "3. **Project points to lower dimensions**:\n",
    "   - In the lower-dimensional space (2D or 3D), we seek a new set of points \\( y_i \\) such that the pairwise similarities between points in the lower-dimensional space are as close as possible to the pairwise similarities in the high-dimensional space.\n",
    "\n",
    "4. **Minimize the Kullback-Leibler (KL) divergence**:\n",
    "   - The algorithm minimizes the Kullback-Leibler (KL) divergence between the probability distributions of the points in the high-dimensional and low-dimensional spaces:\n",
    "   $$ \\text{KL}(P || Q) = \\sum_{i \\neq j} p_{ij} \\log\\left(\\frac{p_{ij}}{q_{ij}}\\right) $$ \n",
    "   where \\( q_{ij} \\) is the similarity between points in the lower-dimensional space (modeled using a Student's t-distribution with one degree of freedom, i.e., a Cauchy distribution).\n",
    "\n",
    "5. **Iterative optimization**:\n",
    "   - t-SNE uses gradient descent to iteratively adjust the positions of the points in the lower-dimensional space to minimize the KL divergence. This results in a mapping of high-dimensional points into a lower-dimensional space that preserves local structures.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "Given a set of data points \\( \\{x_1, x_2, ..., x_N\\} \\) in high-dimensional space \\( \\mathbb{R}^D \\), t-SNE tries to find a lower-dimensional representation \\( \\{y_1, y_2, ..., y_N\\} \\) in \\( \\mathbb{R}^d \\) where \\( d \\ll D \\) (typically \\( d = 2 \\) or \\( d = 3 \\)).\n",
    "\n",
    "### **High-dimensional similarity (pairwise)**:\n",
    "The pairwise similarity between points \\( x_i \\) and \\( x_j \\) is computed using a Gaussian kernel:\n",
    "$$ p_{ij} = \\frac{\\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{\\|x_i - x_k\\|^2}{2\\sigma_i^2}\\right)} $$\n",
    "\n",
    "### **Low-dimensional similarity (pairwise)**:\n",
    "In the low-dimensional space, the pairwise similarity is modeled using a Student's t-distribution with 1 degree of freedom (Cauchy distribution):\n",
    "$$ q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq i} (1 + \\|y_i - y_k\\|^2)^{-1}} $$\n",
    "\n",
    "### **Objective function (KL divergence)**:\n",
    "The objective function that t-SNE seeks to minimize is the Kullback-Leibler divergence between the high-dimensional and low-dimensional distributions:\n",
    "$$ \\text{KL}(P || Q) = \\sum_{i \\neq j} p_{ij} \\log\\left(\\frac{p_{ij}}{q_{ij}}\\right) $$\n",
    "\n",
    "The optimization is typically done using gradient descent, adjusting the positions of the points in the low-dimensional space to minimize the KL divergence.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of t-SNE**\n",
    "\n",
    "1. **Effective for visualization**: t-SNE is highly effective at creating visualizations of high-dimensional datasets, particularly for datasets that exhibit complex, non-linear relationships.\n",
    "\n",
    "2. **Preserves local structure**: t-SNE excels at preserving the local structure of data, meaning that points that are close in the high-dimensional space will remain close in the lower-dimensional space.\n",
    "\n",
    "3. **Arbitrary shapes**: Unlike linear dimensionality reduction techniques (like PCA), t-SNE can handle data with complex, non-linear relationships, which is useful for clustering or classifying data that is not linearly separable.\n",
    "\n",
    "4. **Noise resilience**: t-SNE tends to ignore outliers by focusing on the local structure, which makes it less sensitive to noise than some other techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages of t-SNE**\n",
    "\n",
    "1. **Computationally expensive**: t-SNE can be slow, particularly with large datasets, because it requires computing pairwise similarities and iterative optimization.\n",
    "\n",
    "2. **Difficulty with large datasets**: t-SNE may struggle with very large datasets (thousands of points) as it has a time complexity of \\( O(N^2) \\), where \\( N \\) is the number of data points.\n",
    "\n",
    "3. **Choice of parameters**: The quality of the results can be sensitive to the choice of parameters, especially \\( \\epsilon \\) (perplexity) and the learning rate in the gradient descent. Poor parameter selection can lead to distorted visualizations.\n",
    "\n",
    "4. **Non-interpretable axes**: Unlike PCA, the axes of t-SNE in the lower-dimensional space do not correspond to meaningful features in the original high-dimensional space, making it difficult to interpret the transformed data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example of t-SNE in Python**\n",
    "\n",
    "Here is an example of applying t-SNE to a dataset using the `sklearn` library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', s=50, edgecolors='k')\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE visualization of Iris dataset')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let we have n  data points in a high dimentional space\n",
    "- {X1,X2,....Xn}\n",
    "- tsne operates in unit by unit level. tries to preserve the relation between 2 data points, working on a pair by pair basis whereas pca tries on global level\n",
    "- Given a data point Xi ,whats the probability of picking Xj as its neighbour \n",
    "<i>P(j|i)</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lots of ways to define it mathematically\n",
    "   -  1. one way is to make a function of the distance between the points\n",
    "   - $$ p(j|i) = F( ||Xi-Xj||^2 )$$\n",
    "   - but this formula ignores all other data points\n",
    "   <br><br>\n",
    "   - 2.\n",
    "   -  $$ p(j|i) =F(d_{ij}^2,d_{ik}^2)$$  \n",
    "   -  k!=i,j\n",
    "   -  $d_{ij}$ is the distance between points i,j as above case $d_{ik}$ is the distance to all other data points\n",
    "   - correlation is made -ve for $d_{ij}$ as the distance is bigger lower will be probability to pick as neighbour\n",
    "   - correlation is +ve for $d_{ik}$, as the distance between i and other points increases j is more close to i comparing to others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- above $P_{ij}=((1/N )P_{j/i} +(1/N) P_{i/j} )/2N $\n",
    "- equation contains dynamic standard deviation ,which can be adjusted to include/exclude points near/far.if sigma increased curve become more flat and more data points can be included (here by increasing sigma curve changes to black from red and point on x axis outside red curve get included in black curve )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how to get low dimensional space\n",
    "   - get new dimension(low) Y .preserve similarity\n",
    "   - we do exact same thing with Y ($Q_{ij}$).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tsdimred.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when we try to pack data at a high dimentional space to low dimentional there will not be enough room,\n",
    "- considering normal distribution at the end of curve it becomes 0 ,but we have to extend the tail more to include more data ,ie we are modifying the relation with <i>heavy tailed students t distribution with a digree of freedom == 1 / cauchy distribution </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cauch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kl divergence ->  for comparing two probabilities (P for high dim Q for new low dim).we want P,Q close to each other.whats the level of difference between two distributions\n",
    "- finding Y via gradient descent\n",
    "- kl is loss function\n",
    "- take derivative of loss function withrespect to Yi\n",
    "\n",
    "___\n",
    "\n",
    "- pca considers general trends in data set. what is the direction of maximum variance in data set and will choose it as first principle component... global level . pca is a linear method \n",
    "- tsne is working with local point. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
