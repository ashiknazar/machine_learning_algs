{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique primarily used for the visualization of high-dimensional datasets in a lower-dimensional space (typically 2D or 3D). It is particularly effective at preserving local relationships and is widely used for visualizing high-dimensional data in fields like machine learning, bioinformatics, and image processing.\n",
    "\n",
    "t-SNE works by converting similarities between data points into joint probabilities and aims to minimize the divergence between these probabilities in the higher and lower-dimensional spaces. It is a non-linear technique, meaning it does not assume any specific linear relationship between the dimensions of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **How t-SNE Works**\n",
    "\n",
    "The basic idea behind t-SNE is to:\n",
    "1. **Compute pairwise similarities** in the original high-dimensional space (using Gaussian distributions).\n",
    "2. **Project the data points into a lower-dimensional space** (2D or 3D) while trying to preserve the local structure, i.e., points that are similar in the high-dimensional space should remain close in the low-dimensional space, and dissimilar points should be farther apart.\n",
    "\n",
    "### **Steps of t-SNE**:\n",
    "1. **Compute pairwise similarities in the high-dimensional space**:\n",
    "   - The similarity between two points \\( x_i \\) and \\( x_j \\) is modeled as a conditional probability:\n",
    "   $$ p_{ij} = \\frac{\\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{\\|x_i - x_k\\|^2}{2\\sigma_i^2}\\right)} $$ \n",
    "   where \\( \\|x_i - x_j\\|^2 \\) is the squared Euclidean distance between the points \\( x_i \\) and \\( x_j \\), and \\( \\sigma_i \\) is a parameter controlling the spread of the Gaussian.\n",
    "\n",
    "2. **Convert the similarities to probabilities**:\n",
    "   - The joint probability \\( p_{ij} \\) is symmetric, meaning \\( p_{ij} = p_{ji} \\). The joint probability distribution reflects the likelihood that two points are neighbors in the original high-dimensional space.\n",
    "\n",
    "3. **Project points to lower dimensions**:\n",
    "   - In the lower-dimensional space (2D or 3D), we seek a new set of points \\( y_i \\) such that the pairwise similarities between points in the lower-dimensional space are as close as possible to the pairwise similarities in the high-dimensional space.\n",
    "\n",
    "4. **Minimize the Kullback-Leibler (KL) divergence**:\n",
    "   - The algorithm minimizes the Kullback-Leibler (KL) divergence between the probability distributions of the points in the high-dimensional and low-dimensional spaces:\n",
    "   $$ \\text{KL}(P || Q) = \\sum_{i \\neq j} p_{ij} \\log\\left(\\frac{p_{ij}}{q_{ij}}\\right) $$ \n",
    "   where \\( q_{ij} \\) is the similarity between points in the lower-dimensional space (modeled using a Student's t-distribution with one degree of freedom, i.e., a Cauchy distribution).\n",
    "\n",
    "5. **Iterative optimization**:\n",
    "   - t-SNE uses gradient descent to iteratively adjust the positions of the points in the lower-dimensional space to minimize the KL divergence. This results in a mapping of high-dimensional points into a lower-dimensional space that preserves local structures.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "Given a set of data points \\( \\{x_1, x_2, ..., x_N\\} \\) in high-dimensional space \\( \\mathbb{R}^D \\), t-SNE tries to find a lower-dimensional representation \\( \\{y_1, y_2, ..., y_N\\} \\) in \\( \\mathbb{R}^d \\) where \\( d \\ll D \\) (typically \\( d = 2 \\) or \\( d = 3 \\)).\n",
    "\n",
    "### **High-dimensional similarity (pairwise)**:\n",
    "The pairwise similarity between points \\( x_i \\) and \\( x_j \\) is computed using a Gaussian kernel:\n",
    "$$ p_{ij} = \\frac{\\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{\\|x_i - x_k\\|^2}{2\\sigma_i^2}\\right)} $$\n",
    "\n",
    "### **Low-dimensional similarity (pairwise)**:\n",
    "In the low-dimensional space, the pairwise similarity is modeled using a Student's t-distribution with 1 degree of freedom (Cauchy distribution):\n",
    "$$ q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq i} (1 + \\|y_i - y_k\\|^2)^{-1}} $$\n",
    "\n",
    "### **Objective function (KL divergence)**:\n",
    "The objective function that t-SNE seeks to minimize is the Kullback-Leibler divergence between the high-dimensional and low-dimensional distributions:\n",
    "$$ \\text{KL}(P || Q) = \\sum_{i \\neq j} p_{ij} \\log\\left(\\frac{p_{ij}}{q_{ij}}\\right) $$\n",
    "\n",
    "The optimization is typically done using gradient descent, adjusting the positions of the points in the low-dimensional space to minimize the KL divergence.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of t-SNE**\n",
    "\n",
    "1. **Effective for visualization**: t-SNE is highly effective at creating visualizations of high-dimensional datasets, particularly for datasets that exhibit complex, non-linear relationships.\n",
    "\n",
    "2. **Preserves local structure**: t-SNE excels at preserving the local structure of data, meaning that points that are close in the high-dimensional space will remain close in the lower-dimensional space.\n",
    "\n",
    "3. **Arbitrary shapes**: Unlike linear dimensionality reduction techniques (like PCA), t-SNE can handle data with complex, non-linear relationships, which is useful for clustering or classifying data that is not linearly separable.\n",
    "\n",
    "4. **Noise resilience**: t-SNE tends to ignore outliers by focusing on the local structure, which makes it less sensitive to noise than some other techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages of t-SNE**\n",
    "\n",
    "1. **Computationally expensive**: t-SNE can be slow, particularly with large datasets, because it requires computing pairwise similarities and iterative optimization.\n",
    "\n",
    "2. **Difficulty with large datasets**: t-SNE may struggle with very large datasets (thousands of points) as it has a time complexity of \\( O(N^2) \\), where \\( N \\) is the number of data points.\n",
    "\n",
    "3. **Choice of parameters**: The quality of the results can be sensitive to the choice of parameters, especially \\( \\epsilon \\) (perplexity) and the learning rate in the gradient descent. Poor parameter selection can lead to distorted visualizations.\n",
    "\n",
    "4. **Non-interpretable axes**: Unlike PCA, the axes of t-SNE in the lower-dimensional space do not correspond to meaningful features in the original high-dimensional space, making it difficult to interpret the transformed data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example of t-SNE in Python**\n",
    "\n",
    "Here is an example of applying t-SNE to a dataset using the `sklearn` library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', s=50, edgecolors='k')\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE visualization of Iris dataset')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
