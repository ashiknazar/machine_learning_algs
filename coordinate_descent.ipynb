{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimize function  <b>g</b> <br>\n",
    "$g(w)=g(w_0,w_1,w_2...w_D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- often hard to find minimum for all coordinates ,but easy for each coordinates( when keeping others fixed)\n",
    "- ![](images/coordinate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinate Descent: A Comprehensive Overview\n",
    "\n",
    "## What is Coordinate Descent?\n",
    "\n",
    "Coordinate descent is an iterative optimization algorithm used to minimize a function by successively optimizing along one coordinate direction at a time. It is particularly useful for problems where the optimization is performed over high-dimensional spaces, and is often employed in regularized regression models like Lasso.\n",
    "\n",
    "In coordinate descent, we optimize one variable (coordinate) at a time while keeping the others fixed, and this process is repeated for each coordinate until convergence.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Optimization Problem**: We aim to minimize a function $f(x_1, x_2, \\dots, x_d)$ where $x = (x_1, x_2, \\dots, x_d)$ are the variables.\n",
    "- **Coordinate**: A single variable in the vector $x$.\n",
    "- **Iteration**: The algorithm proceeds by selecting one coordinate at a time, updating it while keeping all other coordinates fixed.\n",
    "\n",
    "## How Coordinate Descent is Calculated\n",
    "\n",
    "Coordinate descent works by solving for each coordinate $x_j$ while fixing the others. The update for each coordinate is performed by minimizing the function $f(x_1, \\dots, x_j, \\dots, x_d)$ with respect to $x_j$, while all other coordinates $x_1, \\dots, x_{j-1}, x_{j+1}, \\dots, x_d$ are held constant.\n",
    "\n",
    "The update rule for each coordinate is given by:\n",
    "\n",
    "$$\n",
    "x_j^{(k+1)} = \\arg \\min_{x_j} f(x_1^{(k)}, x_2^{(k)}, \\dots, x_j, \\dots, x_d^{(k)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_j^{(k)}$ is the value of the coordinate $x_j$ in the $k$-th iteration.\n",
    "- The optimization is done by minimizing the function with respect to $x_j$, while all other coordinates are fixed.\n",
    "\n",
    "### Steps in Coordinate Descent:\n",
    "1. **Initialize**: Start with an initial guess for the coordinates $x_1^{(0)}, x_2^{(0)}, \\dots, x_d^{(0)}$.\n",
    "2. **Update Each Coordinate**: For each $j \\in \\{1, 2, \\dots, d\\}$, update the coordinate $x_j$ by minimizing the function $f(x_1, \\dots, x_j, \\dots, x_d)$ with respect to $x_j$.\n",
    "3. **Repeat**: After updating all coordinates once, repeat the process until the changes in the function values become negligibly small, or until a predetermined number of iterations are reached (i.e., convergence).\n",
    "\n",
    "### Convergence:\n",
    "The algorithm stops when the changes in the variables become sufficiently small or after a fixed number of iterations. In some cases, coordinate descent can converge quickly, particularly for problems where the function is separable with respect to its coordinates.\n",
    "\n",
    "## Example of Coordinate Descent\n",
    "\n",
    "Let's consider a simple quadratic function for illustration:\n",
    "\n",
    "$$\n",
    "f(x, y) = (x - 3)^2 + (y + 2)^2\n",
    "$$\n",
    "\n",
    "We will use coordinate descent to minimize this function.\n",
    "\n",
    "### Step-by-Step Calculation:\n",
    "\n",
    "1. **Initialize**: Let's start with initial guesses for $x$ and $y$. Choose:\n",
    "   $$\n",
    "   x^{(0)} = 0, \\quad y^{(0)} = 0\n",
    "   $$\n",
    "   So, our initial point is $(x_0, y_0) = (0, 0)$.\n",
    "\n",
    "2. **First Iteration**:\n",
    "    - **Update $x$**: Fix $y = 0$, and minimize $f(x, 0) = (x - 3)^2 + (0 + 2)^2 = (x - 3)^2 + 4$.\n",
    "        - To minimize with respect to $x$, take the derivative and set it to zero:\n",
    "        $$\n",
    "        \\frac{\\partial}{\\partial x} \\left[ (x - 3)^2 + 4 \\right] = 2(x - 3)\n",
    "        $$\n",
    "        - Set this equal to zero:\n",
    "        $$\n",
    "        2(x - 3) = 0 \\quad \\Rightarrow \\quad x = 3\n",
    "        $$\n",
    "    - **Update $y$**: Now, fix $x = 3$, and minimize $f(3, y) = (3 - 3)^2 + (y + 2)^2 = (y + 2)^2$.\n",
    "        - To minimize with respect to $y$, take the derivative and set it to zero:\n",
    "        $$\n",
    "        \\frac{\\partial}{\\partial y} \\left[ (y + 2)^2 \\right] = 2(y + 2)\n",
    "        $$\n",
    "        - Set this equal to zero:\n",
    "        $$\n",
    "        2(y + 2) = 0 \\quad \\Rightarrow \\quad y = -2\n",
    "        $$\n",
    "    - After the first iteration, we have $x^{(1)} = 3$ and $y^{(1)} = -2$.\n",
    "\n",
    "3. **Second Iteration**:\n",
    "    - **Update $x$**: Fix $y = -2$, and minimize $f(x, -2) = (x - 3)^2 + (-2 + 2)^2 = (x - 3)^2$.\n",
    "        - The minimum occurs at $x = 3$, so no change is needed.\n",
    "    - **Update $y$**: Fix $x = 3$, and minimize $f(3, y) = (3 - 3)^2 + (y + 2)^2 = (y + 2)^2$.\n",
    "        - The minimum occurs at $y = -2$, so no change is needed.\n",
    "\n",
    "At this point, the algorithm converges because the values of $x$ and $y$ no longer change after the second iteration.\n",
    "\n",
    "### Final Solution:\n",
    "The optimal solution is $x = 3$ and $y = -2$, which corresponds to the minimum point of the function $f(x, y)$.\n",
    "\n",
    "Thus, through coordinate descent, we have minimized the function $f(x, y)$ by iteratively updating each coordinate.\n",
    "\n",
    "## Advantages of Coordinate Descent:\n",
    "- **Simplicity**: The algorithm is easy to implement and computationally efficient, especially for large-scale problems where updates are performed one coordinate at a time.\n",
    "- **Parallelizable**: If the function is separable across coordinates, coordinate descent can be parallelized to speed up the process.\n",
    "- **Sparsity**: In problems like Lasso regression, coordinate descent can efficiently promote sparsity (e.g., forcing some coefficients to zero).\n",
    "\n",
    "## Disadvantages:\n",
    "- **Slow Convergence**: The algorithm can be slow to converge, especially for highly non-convex functions or when the problem is poorly conditioned.\n",
    "- **Local Minima**: In some non-convex optimization problems, coordinate descent can get stuck in local minima, particularly if the starting point is not well chosen.\n",
    "\n",
    "In summary, coordinate descent is an effective optimization technique that updates one coordinate at a time, and is widely used in fields like machine learning where functions can be decomposed into individual coordinates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
