{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Autoencoders**\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "Autoencoders are a type of artificial neural network used for unsupervised learning, primarily for the purpose of dimensionality reduction, feature extraction, and data compression. They consist of two main parts: the **encoder** and the **decoder**. The encoder compresses the input data into a lower-dimensional representation (called the **latent space** or **bottleneck**), while the decoder reconstructs the original input from this compressed form.\n",
    "\n",
    "Autoencoders can be used for a variety of tasks, including anomaly detection, image denoising, data compression, and unsupervised pretraining for deep learning models.\n",
    "\n",
    "---\n",
    "\n",
    "## **Components of an Autoencoder**\n",
    "\n",
    "1. **Encoder**:\n",
    "   - The encoder is the part of the network that takes the input data and reduces it to a smaller representation, known as the **latent space** or **encoding**.\n",
    "   - It typically consists of a series of layers that progressively reduce the dimensionality of the input data.\n",
    "   \n",
    "2. **Latent Space (Bottleneck)**:\n",
    "   - This is the compressed representation of the input data, containing only the most important features. The size of the latent space is a hyperparameter that controls the amount of compression.\n",
    "   \n",
    "3. **Decoder**:\n",
    "   - The decoder takes the compressed data from the latent space and attempts to reconstruct the original input.\n",
    "   - It typically mirrors the encoder's architecture, gradually increasing the dimensionality to match the input shape.\n",
    "\n",
    "4. **Reconstruction**:\n",
    "   - The reconstruction is the output of the decoder. The goal of training is to make this reconstruction as close as possible to the original input.\n",
    "\n",
    "---\n",
    "\n",
    "## **How Autoencoders Work**\n",
    "\n",
    "The basic workflow of an autoencoder is as follows:\n",
    "\n",
    "1. **Input Data**: The network takes an input \\( x \\in \\mathbb{R}^n \\), where \\( n \\) is the dimensionality of the input data.\n",
    "   \n",
    "2. **Encoding Process**: The encoder maps the input \\( x \\) to a lower-dimensional latent representation \\( z \\in \\mathbb{R}^m \\), where \\( m < n \\).\n",
    "\n",
    "3. **Decoding Process**: The decoder takes the encoded representation \\( z \\) and reconstructs the original input \\( \\hat{x} \\), which is ideally as close as possible to \\( x \\).\n",
    "\n",
    "4. **Loss Function**: The network minimizes the loss between the original input \\( x \\) and the reconstructed output \\( \\hat{x} \\). Common loss functions include **mean squared error (MSE)** or **binary cross-entropy**, depending on the type of data (continuous or binary).\n",
    "\n",
    "   The loss function is typically expressed as:\n",
    "   $$ \\mathcal{L}(x, \\hat{x}) = \\|x - \\hat{x}\\|^2 $$ \n",
    "   where \\( \\hat{x} \\) is the reconstructed data and \\( x \\) is the original input.\n",
    "\n",
    "5. **Optimization**: During training, the model learns to minimize this reconstruction error by adjusting the weights of the encoder and decoder using optimization techniques like **gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Types of Autoencoders**\n",
    "\n",
    "1. **Vanilla Autoencoders**:\n",
    "   - The standard form of autoencoders, consisting of a simple feedforward network for both the encoder and the decoder.\n",
    "\n",
    "2. **Convolutional Autoencoders**:\n",
    "   - Used for image data, these autoencoders use convolutional layers in the encoder and decoder, making them more effective at capturing spatial hierarchies in the data.\n",
    "   \n",
    "3. **Variational Autoencoders (VAEs)**:\n",
    "   - A probabilistic version of autoencoders, VAEs model the data distribution and introduce a regularization term to make the latent space more structured (i.e., Gaussian). This makes VAEs useful for generative tasks like image generation.\n",
    "\n",
    "4. **Denoising Autoencoders**:\n",
    "   - These autoencoders are trained to reconstruct the original input from noisy versions of the data. They are useful for tasks like image denoising or speech enhancement.\n",
    "\n",
    "5. **Sparse Autoencoders**:\n",
    "   - These autoencoders introduce a sparsity constraint on the latent space, forcing the model to learn a compact representation where only a few neurons are active at a time.\n",
    "\n",
    "6. **Contractive Autoencoders**:\n",
    "   - These autoencoders add a penalty term to the loss function that encourages the network to learn more robust representations by making the encoding process less sensitive to small changes in the input.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "Given a dataset \\( X = \\{x_1, x_2, ..., x_N\\} \\), an autoencoder seeks to find a function \\( f: X \\to \\hat{X} \\), where \\( \\hat{x} \\) is the reconstructed data point.\n",
    "\n",
    "The encoder maps the input data \\( x \\) to a latent vector \\( z \\):\n",
    "$$ z = f_{\\text{encoder}}(x) $$\n",
    "\n",
    "The decoder then reconstructs the input from the latent vector:\n",
    "$$ \\hat{x} = f_{\\text{decoder}}(z) $$\n",
    "\n",
    "The model minimizes the reconstruction loss:\n",
    "$$ \\mathcal{L}(x, \\hat{x}) = \\|x - \\hat{x}\\|^2 $$\n",
    "\n",
    "In the case of **variational autoencoders (VAE)**, the encoder approximates the posterior distribution \\( p(z|x) \\) by learning to output parameters (mean and variance) of a Gaussian distribution in the latent space, and the decoder tries to reconstruct \\( x \\) by sampling from this distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## **Training an Autoencoder**\n",
    "\n",
    "Training an autoencoder typically involves the following steps:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "   - Normalize or standardize the input data, especially if the data varies in scale or units (e.g., images or tabular data).\n",
    "\n",
    "2. **Architecture Design**:\n",
    "   - Choose the number of layers and units in both the encoder and decoder.\n",
    "   - Select the activation functions (e.g., ReLU, sigmoid, or tanh) for each layer.\n",
    "\n",
    "3. **Loss Function**:\n",
    "   - Choose an appropriate loss function, commonly **mean squared error (MSE)** or **binary cross-entropy**, depending on the type of input data.\n",
    "\n",
    "4. **Optimization**:\n",
    "   - Use optimization algorithms like **Stochastic Gradient Descent (SGD)**, **Adam**, or **RMSprop** to minimize the loss function.\n",
    "\n",
    "5. **Regularization** (optional):\n",
    "   - You can add regularization terms like L1 or L2 regularization to prevent overfitting, or use techniques like **dropout**.\n",
    "\n",
    "6. **Training**:\n",
    "   - Train the model using backpropagation to update the weights of the encoder and decoder.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications of Autoencoders**\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   - Autoencoders can be used as a non-linear dimensionality reduction technique, similar to PCA. They can compress the data into a lower-dimensional latent space for visualization or further processing.\n",
    "\n",
    "2. **Anomaly Detection**:\n",
    "   - By training an autoencoder on a \"normal\" dataset, the reconstruction error can be used to detect anomalies. High reconstruction errors indicate that the data point is significantly different from the learned patterns, thus identifying potential outliers.\n",
    "\n",
    "3. **Denoising**:\n",
    "   - Denoising autoencoders are trained to clean up noisy data. These models are widely used in image processing, such as removing noise from images or video.\n",
    "\n",
    "4. **Image Compression**:\n",
    "   - Autoencoders are effective in image compression tasks, as the encoder learns to compress the data while the decoder reconstructs it, similar to how traditional compression algorithms work.\n",
    "\n",
    "5. **Generative Models**:\n",
    "   - Variational Autoencoders (VAEs) are used to generate new data, such as creating new images, audio, or even text by sampling from the learned latent space.\n",
    "\n",
    "6. **Pretraining**:\n",
    "   - Autoencoders can be used as a pretraining technique for deep learning models. The encoder part of the autoencoder can serve as a feature extractor, providing useful representations for other tasks like classification or regression.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example of Autoencoder in Python (Using Keras)**\n",
    "\n",
    "Hereâ€™s a simple example of training an autoencoder on the **MNIST dataset** using Keras:\n",
    "\n",
    "```python\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = x_train.reshape((x_train.shape[0], np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((x_test.shape[0], np.prod(x_test.shape[1:])))\n",
    "\n",
    "# Define the autoencoder model\n",
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(64, activation='relu')(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# Define the encoder model\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
    "\n",
    "# Visualize the encoded and decoded images\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10  # number of digits to display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
