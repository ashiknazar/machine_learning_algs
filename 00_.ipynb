{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning Algorithms**\n",
    "\n",
    "## **1. Supervised Learning**\n",
    "Supervised learning involves training a model on labeled data, where each data point has input features (**X**) and a corresponding output label (**y**).\n",
    "\n",
    "### **Goals**:\n",
    "- **Classification**: Predict discrete labels (e.g., spam vs. not spam).\n",
    "- **Regression**: Predict continuous values (e.g., house prices).\n",
    "\n",
    "### **Common Algorithms**:\n",
    "\n",
    "#### **Classification**\n",
    "1. **Logistic Regression**: Predicts probabilities using a sigmoid function.\n",
    "2. **Decision Trees**: Splits data based on decision rules.\n",
    "3. **Random Forest**: An ensemble of decision trees to reduce overfitting.\n",
    "4. **Support Vector Machines (SVM)**: Finds the optimal hyperplane to separate classes.\n",
    "5. **k-Nearest Neighbors (k-NN)**: Classifies based on the majority class of k-nearest neighbors.\n",
    "6. **Neural Networks (NN)**: Mimics the human brain; suitable for complex patterns (e.g., image recognition).\n",
    "\n",
    "#### **Regression**\n",
    "1. **Linear Regression**: Fits a straight line to predict continuous values.\n",
    "2. **Polynomial Regression**: Captures non-linear relationships by fitting higher-degree polynomials.\n",
    "3. **Ridge & Lasso Regression**: Regularized versions of linear regression to prevent overfitting.\n",
    "4. **Gradient Boosting**: Builds models sequentially to minimize residual errors (e.g., XGBoost, LightGBM).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Unsupervised Learning**\n",
    "Unsupervised learning works with unlabeled data to uncover hidden patterns or structures.\n",
    "\n",
    "### **Goals**:\n",
    "- **Clustering**: Group similar data points.\n",
    "- **Dimensionality Reduction**: Reduce features while retaining significant patterns.\n",
    "\n",
    "### **Common Algorithms**:\n",
    "\n",
    "#### **Clustering**\n",
    "1. **k-Means Clustering**: Partitions data into k clusters by minimizing intra-cluster variance.\n",
    "2. **Hierarchical Clustering**: Builds a tree-like structure to group data (agglomerative or divisive).\n",
    "3. **DBSCAN**: Groups data based on density, handles noise effectively.\n",
    "4. **Gaussian Mixture Models (GMM)**: Fits overlapping clusters using probabilities.\n",
    "\n",
    "#### **Dimensionality Reduction**\n",
    "1. **Principal Component Analysis (PCA)**: Projects data onto principal components for feature reduction.\n",
    "2. **t-SNE**: Non-linear technique for visualizing high-dimensional data.\n",
    "3. **Autoencoders**: Neural networks for learning compact data representations.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison of Supervised vs. Unsupervised Learning**\n",
    "\n",
    "| Aspect                | Supervised Learning                          | Unsupervised Learning                     |\n",
    "|-----------------------|---------------------------------------------|------------------------------------------|\n",
    "| **Input Data**         | Labeled (X, y)                              | Unlabeled (X)                            |\n",
    "| **Goal**               | Predict labels or outcomes                  | Find patterns or clusters                |\n",
    "| **Examples**           | Logistic Regression, SVM                   | k-Means, PCA, DBSCAN                     |\n",
    "| **Evaluation Metrics** | Accuracy, F1-score, RMSE                    | Cluster quality, silhouette score        |\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications**\n",
    "| Algorithm Type         | Use Cases                                    |\n",
    "|-----------------------|---------------------------------------------|\n",
    "| **Supervised Learning** | Fraud detection, sentiment analysis, medical diagnosis, image classification. |\n",
    "| **Unsupervised Learning** | Customer segmentation, anomaly detection, recommendation systems, gene analysis. |\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Tips**\n",
    "- Start with simple datasets like the Iris dataset for classification or Boston housing for regression.\n",
    "- Use visualization techniques like scatter plots and dendrograms to understand clustering results.\n",
    "- Explore pre-built libraries like **scikit-learn** for rapid prototyping of machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
