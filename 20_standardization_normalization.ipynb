{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf54b3f",
   "metadata": {},
   "source": [
    "# 📘 Notes: Encoding, Scaling, and Feature Use in ML Models\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 One-Hot vs Label Encoding\n",
    "\n",
    "- Use **One-Hot Encoding** when:\n",
    "  - You have **nominal** categories (no inherent order)\n",
    "  - Especially for **linear models** (Linear Regression, Logistic Regression, etc.)\n",
    "- Use **Label Encoding** when:\n",
    "  - Categories have an **ordinal relationship**\n",
    "  - Or for **tree-based models** (Decision Trees, Random Forests, XGBoost)\n",
    "\n",
    "> 💡 Tree models split based on feature values, not their scale, so label encoding usually works fine.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 Linear Models & Label Encoding\n",
    "\n",
    "- Linear models interpret label-encoded values **numerically** (i.e., 0, 1, 2... as having increasing influence).\n",
    "- If category values are unordered, this can introduce a **false sense of order**, which can mislead the model.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 One-Hot Encoding in Tree Models?\n",
    "\n",
    "- It **also works**, and sometimes improves performance with high-cardinality categories.\n",
    "- However, it increases dimensionality.\n",
    "- Label encoding is **simpler** and often sufficient for trees since they split on specific values anyway.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 Standardization vs Normalization\n",
    "\n",
    "| Method            | Formula                                      | When to Use                                        |\n",
    "|-------------------|----------------------------------------------|----------------------------------------------------|\n",
    "| **Standardization** | \\( z = \\frac{x - \\mu}{\\sigma} \\)             | Data is **roughly normal**; for linear models, SVM, PCA |\n",
    "| **Normalization**   | \\( x_{norm} = \\frac{x - min}{max - min} \\)  | When using **distance-based** models (KNN, KMeans), neural networks |\n",
    "\n",
    "- **Standardization** preserves the shape of the distribution.\n",
    "- **Normalization** rescales features to a common range (usually [0,1]).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 Why Use Min-Max Scaling?\n",
    "\n",
    "To make features **comparable** in scale, especially when:\n",
    "- Using **KNN**, **KMeans**, or **Neural Networks**\n",
    "- Preventing features with large ranges from dominating the model\n",
    "\n",
    "> Real-life example: Income (0–100,000) will dominate age (0–100) unless both are scaled.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 Binary Features like Gender in Models\n",
    "\n",
    "| Model Type        | Use Gender as 0/1? | Explanation |\n",
    "|-------------------|-------------------|-------------|\n",
    "| Tree-based Models | ✅ Yes            | Uses value for splitting (e.g., `gender == 1`) |\n",
    "| Linear Regression | ✅ Yes            | Learns effect via coefficient `β₁` in `y = β₀ + β₁ * gender` |\n",
    "\n",
    "> If gender = 1 means male, then `β₁` is the average difference in `y` between males and females.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 Categorical Features with >2 Categories\n",
    "\n",
    "- **Tree models**: Label or One-Hot both are fine.\n",
    "- **Linear models**: Use One-Hot Encoding to avoid implying an order.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1497e55",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
