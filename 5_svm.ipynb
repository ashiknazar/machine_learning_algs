{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support Vector Machine (SVM)**\n",
    "\n",
    "Support Vector Machine (SVM) is a powerful supervised learning algorithm commonly used for classification tasks, but it can also be applied to regression problems. SVM works by finding the optimal hyperplane that best separates the data points into different classes.\n",
    "\n",
    "---\n",
    "\n",
    "## **Basic Concepts**\n",
    "\n",
    "- **Hyperplane**: A hyperplane is a decision boundary that separates the data points of different classes. In a 2D space, this is a line, in 3D, it is a plane, and in higher dimensions, it is a hyperplane.\n",
    "\n",
    "- **Support Vectors**: These are the data points that are closest to the hyperplane. The position of the hyperplane is determined by these support vectors. They are the most critical points for the classification process.\n",
    "\n",
    "- **Margin**: The margin is the distance between the hyperplane and the closest data point on either side. SVM aims to maximize this margin, as a larger margin is generally associated with better generalization to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Working of SVM**\n",
    "\n",
    "SVM tries to find a hyperplane that best divides the dataset into two classes, such that the margin between the classes is maximized. This is achieved by solving an optimization problem that minimizes classification errors while maximizing the margin.\n",
    "\n",
    "### **Linear SVM**\n",
    "\n",
    "For linearly separable data (where a straight line or hyperplane can perfectly separate the classes), SVM finds the hyperplane that maximizes the margin. The objective is to solve:\n",
    "\n",
    "$$\n",
    "\\text{maximize } \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "Where \\( w \\) is the weight vector that defines the orientation of the hyperplane.\n",
    "\n",
    "- The decision boundary or hyperplane is given by:\n",
    "\n",
    "$$\n",
    "w \\cdot x + b = 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( w \\) is the weight vector,\n",
    "- \\( b \\) is the bias term,\n",
    "- \\( x \\) is the feature vector.\n",
    "\n",
    "### **Non-Linear SVM**\n",
    "\n",
    "For non-linearly separable data, SVM can be extended by using a **kernel trick** to map the data into higher dimensions where a linear hyperplane can separate the data points.\n",
    "\n",
    "#### **Kernel Trick**\n",
    "\n",
    "A kernel is a function that computes the inner product of two data points in a higher-dimensional space without actually computing the transformation. Common kernels include:\n",
    "\n",
    "- **Linear Kernel**: No transformation, used for linearly separable data.\n",
    "  \n",
    "  $$ K(x, x') = x \\cdot x' $$\n",
    "\n",
    "- **Polynomial Kernel**: Maps data into a higher-dimensional space using polynomial functions.\n",
    "  \n",
    "  $$ K(x, x') = (x \\cdot x' + c)^d $$\n",
    "\n",
    "- **Radial Basis Function (RBF) or Gaussian Kernel**: Maps data into an infinite-dimensional space. It is the most commonly used kernel in SVM.\n",
    "\n",
    "  $$ K(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\right) $$\n",
    "\n",
    "- **Sigmoid Kernel**: Uses a sigmoid function, similar to a neural network activation function.\n",
    "\n",
    "  $$ K(x, x') = \\tanh(\\alpha x \\cdot x' + c) $$\n",
    "\n",
    "### **Soft Margin SVM**\n",
    "\n",
    "In practice, real-world data may not be perfectly separable, so **soft margin SVM** allows some misclassification. It introduces a penalty parameter \\( C \\) to the optimization objective that controls the trade-off between maximizing the margin and minimizing classification errors.\n",
    "\n",
    "The objective becomes:\n",
    "\n",
    "$$\n",
    "\\text{minimize } \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{N} \\xi_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\xi_i \\) are slack variables representing misclassified data points.\n",
    "- \\( C \\) is a regularization parameter controlling the trade-off between margin size and classification error.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of SVM**\n",
    "\n",
    "- **Effective in high-dimensional spaces**: SVM performs well even when the number of features exceeds the number of data points.\n",
    "- **Memory efficient**: SVM uses only a subset of the training data (support vectors) to make decisions, making it memory efficient.\n",
    "- **Versatile**: Through the kernel trick, SVM can handle non-linearly separable data.\n",
    "- **Robust to overfitting**: Especially in high-dimensional spaces, due to the regularization parameter \\( C \\) and the margin maximization approach.\n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages of SVM**\n",
    "\n",
    "- **Computationally expensive**: Training time can be high, especially with large datasets and complex kernels, as it requires solving quadratic programming problems.\n",
    "- **Not suitable for larger datasets**: SVM may not scale well with a large number of data points or features, especially for non-linear kernels.\n",
    "- **Sensitive to the choice of parameters**: The choice of kernel, regularization parameter \\( C \\), and kernel-specific parameters (like \\( \\sigma \\) in RBF) needs careful tuning.\n",
    "- **Interpretability**: SVM is considered a black-box model, as it can be hard to interpret the results and understand the decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "## **SVM Applications**\n",
    "\n",
    "- **Classification**: SVM is commonly used in binary classification tasks like spam detection, image classification, and sentiment analysis.\n",
    "- **Regression (SVR)**: SVM can also be used for regression, where it tries to find a hyperplane that best fits the data while minimizing errors. The objective is to minimize the margin of error instead of the classification margin.\n",
    "- **Anomaly Detection**: SVM can detect outliers by identifying points that do not conform to the general pattern of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example**\n",
    "\n",
    "Given the points \\( P = (2, 3) \\) and \\( Q = (4, 5) \\), SVM will:\n",
    "\n",
    "- Find the optimal hyperplane that separates the points of class 1 and class 2.\n",
    "- If the data is linearly separable, it will place the hyperplane that maximizes the margin.\n",
    "- If the data is not linearly separable, it will map the data to a higher-dimensional space using a kernel (e.g., RBF) and find the hyperplane in that space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "Support Vector Machines (SVM) are powerful classifiers and regressors that are effective for both linear and non-linear problems. By maximizing the margin and using kernels, SVM can perform well even in complex, high-dimensional datasets. However, SVM can be computationally expensive and sensitive to hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm](images/svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
