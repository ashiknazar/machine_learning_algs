{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hierarchical Clustering**\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "Hierarchical clustering is an unsupervised machine learning algorithm used to group similar objects into clusters. Unlike K-Means clustering, which requires specifying the number of clusters in advance, hierarchical clustering creates a hierarchy of clusters by either successively merging smaller clusters (agglomerative) or successively splitting a larger cluster (divisive). It generates a dendrogram, a tree-like diagram that represents the merging or splitting process.\n",
    "\n",
    "Hierarchical clustering can be divided into two main types:\n",
    "1. **Agglomerative Hierarchical Clustering (Bottom-up approach)**\n",
    "2. **Divisive Hierarchical Clustering (Top-down approach)**\n",
    "\n",
    "The most commonly used method is **Agglomerative Hierarchical Clustering**, where each data point starts in its own cluster, and pairs of clusters are merged as the algorithm progresses.\n",
    "\n",
    "---\n",
    "\n",
    "## **How Hierarchical Clustering Works**\n",
    "\n",
    "### **1. Agglomerative (Bottom-up) Approach**\n",
    "\n",
    "In agglomerative hierarchical clustering, each data point is initially considered as its own cluster. The algorithm then performs the following steps iteratively:\n",
    "1. **Compute the proximity matrix**: Calculate the distances (or similarities) between all pairs of data points.\n",
    "2. **Merge the closest clusters**: Identify the two clusters with the smallest distance between them and merge them into a single cluster.\n",
    "3. **Update the proximity matrix**: Recompute the proximity matrix by considering the new cluster formed and its distance to other clusters.\n",
    "4. **Repeat the process**: Continue merging the closest clusters until all data points belong to a single cluster.\n",
    "\n",
    "### **2. Divisive (Top-down) Approach**\n",
    "\n",
    "Divisive hierarchical clustering begins with all data points in a single cluster. The algorithm then recursively splits the cluster into smaller clusters until each data point is in its own cluster or a predefined stopping criterion is met. While less commonly used, this method can be more computationally expensive than the agglomerative approach.\n",
    "\n",
    "---\n",
    "\n",
    "## **Distance Measures**\n",
    "\n",
    "The choice of distance measure significantly impacts the clustering process. Common distance metrics used in hierarchical clustering include:\n",
    "- **Euclidean Distance**: The straight-line distance between two points in the feature space.\n",
    "- **Manhattan Distance**: The sum of the absolute differences between the coordinates of two points.\n",
    "- **Cosine Similarity**: A measure of similarity between two vectors based on their angle (used for text data or when working with vectors).\n",
    "\n",
    "---\n",
    "\n",
    "## **Linkage Criteria**\n",
    "\n",
    "The linkage criterion defines how the distance between clusters is calculated. Some common linkage methods include:\n",
    "\n",
    "1. **Single Linkage (Nearest Point Linkage)**:\n",
    "   - The distance between two clusters is the shortest distance between any single member of one cluster and any single member of the other cluster. \n",
    "   - This method can result in long, \"chained\" clusters, making it sensitive to noise.\n",
    "\n",
    "   $$ \\text{Distance between clusters} = \\min(d(a,b)) $$\n",
    "\n",
    "2. **Complete Linkage (Farthest Point Linkage)**:\n",
    "   - The distance between two clusters is the longest distance between any single member of one cluster and any single member of the other cluster. \n",
    "   - This method tends to create compact clusters.\n",
    "\n",
    "   $$ \\text{Distance between clusters} = \\max(d(a,b)) $$\n",
    "\n",
    "3. **Average Linkage**:\n",
    "   - The distance between two clusters is the average of the distances between all pairs of points in the two clusters.\n",
    "\n",
    "   $$ \\text{Distance between clusters} = \\frac{1}{n_1n_2} \\sum_{i=1}^{n_1} \\sum_{j=1}^{n_2} d(x_i, y_j) $$\n",
    "\n",
    "4. **Wardâ€™s Linkage (Minimum Variance)**:\n",
    "   - This method minimizes the total within-cluster variance when merging clusters. It generally leads to more balanced clusters.\n",
    "\n",
    "   $$ \\text{Distance between clusters} = \\sqrt{\\frac{(n_1+n_2)}{n_1n_2}(||C_1 - C_2||^2)} $$\n",
    "\n",
    "---\n",
    "\n",
    "## **Dendrogram**\n",
    "\n",
    "A **dendrogram** is a tree-like diagram that visually represents the hierarchical clustering process. It shows the order in which clusters are merged (or split) and the distance at which the merging occurs. The vertical axis of the dendrogram represents the distance or dissimilarity between clusters, while the horizontal axis represents individual data points or clusters.\n",
    "\n",
    "### **Interpreting the Dendrogram**\n",
    "- The **height** of the branches represents the distance or dissimilarity between clusters. The lower the height, the more similar the clusters are.\n",
    "- A **cut-off point** can be chosen by drawing a horizontal line across the dendrogram at a specific height to determine the number of clusters. The clusters formed below this line are the final clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of Hierarchical Clustering**\n",
    "\n",
    "- **No need to specify the number of clusters** in advance.\n",
    "- **Produces a dendrogram**, which provides a detailed view of the data structure and the merging process.\n",
    "- **Works well for small datasets** and for cases where the number of clusters is not known.\n",
    "- Can identify clusters of different shapes and sizes, unlike K-Means, which assumes spherical clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages of Hierarchical Clustering**\n",
    "\n",
    "- **Computationally expensive**: The algorithm has a time complexity of \\(O(n^2)\\) for the agglomerative approach, making it less efficient for large datasets.\n",
    "- **Sensitive to noise and outliers**: Especially with single linkage, outliers can distort the results.\n",
    "- **Not scalable to very large datasets**: Hierarchical clustering becomes impractical for datasets with millions of data points.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications of Hierarchical Clustering**\n",
    "\n",
    "- **Gene expression data analysis**: Identifying groups of genes with similar expression patterns.\n",
    "- **Customer segmentation**: Grouping customers based on purchasing behavior.\n",
    "- **Document clustering**: Grouping similar documents or topics together.\n",
    "- **Image segmentation**: Segmenting images into regions with similar properties.\n",
    "  \n",
    "---\n",
    "\n",
    "## **Example of Hierarchical Clustering in Python**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=10, centers=3, random_state=42)\n",
    "\n",
    "# Perform agglomerative clustering\n",
    "model = AgglomerativeClustering(n_clusters=3)\n",
    "y_pred = model.fit_predict(X)\n",
    "\n",
    "# Plotting the dendrogram\n",
    "linked = linkage(X, 'single')\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
